{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "    from pandas.api.types import is_categorical_dtype\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            # skip datetime type or categorical type\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_label_encode(df, subject_cols):\n",
    "    #from sklearn.preprocessing import LabelEncoder\n",
    "    #lbl = LabelEncoder()\n",
    "    for str_col in subject_cols:\n",
    "        # ===== assumes Series of string =====\n",
    "        temp_dict = {value: i for i, value in enumerate(df[str_col].unique())}\n",
    "        df[str_col] = (df[str_col].map(temp_dict)).astype(np.int16)\n",
    "    del temp_dict, str_col; gc.collect()\n",
    "        #lbl.fit(df[str_col].unique())\n",
    "        #df[str_col] = lbl.transform(df[str_col])\n",
    "        #test_df[str_col] = lbl.transform(test_df[str_col])\n",
    "    #del lbl, str_col; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_freq_encode(df, str_col):\n",
    "    temp_dict = {sample: df.loc[df[str_col]==sample].shape[0] for sample in df[str_col].unique()}\n",
    "    #for sample in df[str_col].unique():\n",
    "    #    temp_dict[sample] = df.loc[df[str_col]==sample].shape[0]\n",
    "    df[f'{str_col}_ratio'] = df[str_col].map(temp_dict).astype(np.int32) / df[str_col].shape[0]\n",
    "    del temp_dict; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cyclical(df, str_col):\n",
    "    # e.g. df['hr'] = df.timestamp.dt.hour; apply_cyclical(df, 'hr')\n",
    "    # ===== assumes integer array =====\n",
    "    # ===== assumes min and max exists in array =====\n",
    "    temp = pd.DataFrame()\n",
    "    temp['unique_sorted'] = (df[str_col] - df[str_col].min()).sort_values().unique()\n",
    "    int_max = temp.unique_sorted.max()\n",
    "    temp['sin'] = np.sin(2 * np.pi * temp.unique_sorted / int_max)\n",
    "    temp['cos'] = np.cos(2 * np.pi * temp.unique_sorted / int_max)\n",
    "    temp = temp.set_index('unique_sorted')\n",
    "    df[f'{str_col}_sin'] = (df[str_col] - df[str_col].min()).map(temp.sin)\n",
    "    df[f'{str_col}_cos'] = (df[str_col] - df[str_col].min()).map(temp.cos)\n",
    "    del temp, int_max; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_target_encode(df, test_df, target_col, cat_col, smooth=False, m=0, statistic=False, print_option=False):\n",
    "    \n",
    "    df_group = df.groupby(cat_col)[target_col]\n",
    "    group_mean = df_group.mean().astype(np.float16)\n",
    "    \n",
    "    if smooth:\n",
    "        global_mean = df[target_col].mean()\n",
    "        group_count = df_group.count().astype(np.float16)\n",
    "        smoother = ((group_count * group_mean) + (m * global_mean)) / (group_count + m)\n",
    "        df[f'{cat_col}_smoothed_mean'] = df[f'{cat_col}'].map(smoother)\n",
    "        test_df[f'{cat_col}_smoothed_mean'] = test_df[f'{cat_col}'].map(smoother)\n",
    "        del global_mean, group_count, smoother; gc.collect()\n",
    "    else:\n",
    "        df[f'{cat_col}_mean'] = df[f'{cat_col}'].map(group_mean)\n",
    "        test_df[f'{cat_col}_mean'] = test_df[f'{cat_col}'].map(group_mean)\n",
    "    \n",
    "    if statistic:\n",
    "        group_min = df_group.min().astype(np.float16)\n",
    "        group_max = df_group.max().astype(np.float16)\n",
    "        group_std = df_group.std().astype(np.float16)\n",
    "        df[f'{cat_col}_min'] = df[f'{cat_col}'].map(group_min)\n",
    "        test_df[f'{cat_col}_min'] = df[f'{cat_col}'].map(group_min)\n",
    "        df[f'{cat_col}_max'] = df[f'{cat_col}'].map(group_max)\n",
    "        test_df[f'{cat_col}_max'] = df[f'{cat_col}'].map(group_max)\n",
    "        df[f'{cat_col}_std'] = df[f'{cat_col}'].map(group_std)\n",
    "        test_df[f'{cat_col}_std'] = df[f'{cat_col}'].map(group_std)\n",
    "        df[f'{cat_col}_range'] = df[f'{cat_col}_max'] - df[f'{cat_col}_min']\n",
    "        test_df[f'{cat_col}_range'] = df[f'{cat_col}_max'] - df[f'{cat_col}_min']\n",
    "        group_Q1 = df_group.quantile(0.25).astype(np.float16)\n",
    "        group_Q2 = df_group.median().astype(np.float16)\n",
    "        group_Q3 = df_group.quantile(0.75).astype(np.float16)\n",
    "        df[f'{cat_col}_Q1'] = df[f'{cat_col}'].map(group_Q1)\n",
    "        test_df[f'{cat_col}_Q1'] = test_df[f'{cat_col}'].map(group_Q1)\n",
    "        df[f'{cat_col}_Q2'] = df[f'{cat_col}'].map(group_Q2)\n",
    "        train_df[f'{cat_col}_Q2'] = train_df[f'{cat_col}'].map(group_Q2)\n",
    "        df[f'{cat_col}_Q3'] = df[f'{cat_col}'].map(group_Q3)\n",
    "        test_df[f'{cat_col}_Q3'] = test_df[f'{cat_col}'].map(group_Q3)\n",
    "        df[f'{cat_col}_IQR'] = df[f'{cat_col}_Q1'] - df[f'{cat_col}_Q3']\n",
    "        test_df[f'{cat_col}_IQR'] = test_df[f'{cat_col}_Q1'] - test_df[f'{cat_col}_Q3']\n",
    "        del group_min, group_max, group_std, group_Q1, group_Q2, group_Q3; gc.collect()\n",
    "        \n",
    "    if print_option:\n",
    "        print(\"Generated features: apply_target_encode\")\n",
    "        if smooth:\n",
    "            print(f\"'{cat_col}_smoothed_mean',\")\n",
    "        elif smooth == False:\n",
    "            print(f\"'{cat_col}_mean',\")\n",
    "        if statistic:\n",
    "            print(f\"'{cat_col}_min',\")\n",
    "            print(f\"'{cat_col}_max',\")\n",
    "            print(f\"'{cat_col}_std',\")\n",
    "            print(f\"'{cat_col}_range',\")\n",
    "            print(f\"'{cat_col}_Q1',\")\n",
    "            print(f\"'{cat_col}_Q2',\")\n",
    "            print(f\"'{cat_col}_Q3',\")\n",
    "            print(f\"'{cat_col}_IQR',\")\n",
    "    \n",
    "    del df_group, group_mean; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv'); reduce_mem_usage(train_df)\n",
    "meta_user = pd.read_csv('user.csv'); reduce_mem_usage(meta_user)\n",
    "meta_account = pd.read_csv('account.csv'); reduce_mem_usage(meta_account)\n",
    "meta_access = pd.read_csv('access.csv'); reduce_mem_usage(meta_access)\n",
    "\n",
    "sample_submission = pd.read_csv('answer_sample.csv')\n",
    "test_foruser = pd.read_csv('test_foruser.csv'); reduce_mem_usage(test_foruser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_df_pre(t_df, test_foruser):\n",
    "    # label encode\n",
    "    # apply_label_encode(t_df, ['gender', 'family_structure', 'premium_type'])\n",
    "\n",
    "    # freq encode\n",
    "    apply_freq_encode(t_df, 'birth_year')\n",
    "    apply_freq_encode(t_df, 'family_structure')\n",
    "    apply_freq_encode(t_df, 'gender')\n",
    "    apply_freq_encode(t_df, 'premium_type')\n",
    "    apply_freq_encode(test_foruser, 'birth_year')\n",
    "    apply_freq_encode(test_foruser, 'family_structure')\n",
    "    apply_freq_encode(test_foruser, 'gender')\n",
    "    apply_freq_encode(test_foruser, 'premium_type')\n",
    "    \n",
    "    # cyclical date feature\n",
    "    for col in ['registered', 'premium_registered']:\n",
    "        f = lambda x: x.split(' ')[0]\n",
    "        t_df[f'{col}_at_date'] = pd.to_datetime(t_df[f'{col}_at'].apply(f), format='%Y-%m-%d')\n",
    "        test_foruser[f'{col}_at_date'] = pd.to_datetime(test_foruser[f'{col}_at'].apply(f), format='%Y-%m-%d')\n",
    "        \n",
    "        t_df[f'{col}_at_day'] = t_df[f'{col}_at_date'].dt.day.astype(np.int8)\n",
    "        test_foruser[f'{col}_at_day'] = test_foruser[f'{col}_at_date'].dt.day.astype(np.int8)\n",
    "        # apply_target_encode(train_df, test_foruser, 'premium_expired', f'{col}_at_day', smooth=True, m=10)\n",
    "        \n",
    "        t_df[f'{col}_at_month'] = t_df[f'{col}_at_date'].dt.month.astype(np.int8)\n",
    "        test_foruser[f'{col}_at_month'] = test_foruser[f'{col}_at_date'].dt.month.astype(np.int8)\n",
    "        apply_target_encode(train_df, test_foruser, 'premium_expired', f'{col}_at_month', smooth=True, m=6)\n",
    "        \n",
    "        t_df[f'{col}_at_year'] = t_df[f'{col}_at_date'].dt.year.astype(np.int8)\n",
    "        test_foruser[f'{col}_at_year'] = test_foruser[f'{col}_at_date'].dt.year.astype(np.int8)\n",
    "        apply_target_encode(train_df, test_foruser, 'premium_expired', f'{col}_at_year', smooth=True, m=4)\n",
    "        \n",
    "        apply_cyclical(t_df, f'{col}_at_day')\n",
    "        apply_cyclical(t_df, f'{col}_at_month')\n",
    "        #apply_cyclical(t_df, f'{col}_at_year')\n",
    "        apply_cyclical(test_foruser, f'{col}_at_day')\n",
    "        apply_cyclical(test_foruser, f'{col}_at_month')\n",
    "        #apply_cyclical(test_foruser, f'{col}_at_year')\n",
    "        \n",
    "        #del t_df[f'{col}_at_date'], test_foruser[f'{col}_at_date']#, t_df[f'{col}_at_day'], t_df[f'{col}_at_month'], t_df[f'{col}_at_year']\n",
    "            \n",
    "        f = lambda x: x.split(' ')[1]\n",
    "        t_df[f'{col}_at_hour'] = t_df[f'{col}_at'].apply(f).astype(np.int8)\n",
    "        test_foruser[f'{col}_at_hour'] = test_foruser[f'{col}_at'].apply(f).astype(np.int8)\n",
    "        apply_cyclical(t_df, f'{col}_at_hour')\n",
    "        apply_cyclical(test_foruser, f'{col}_at_hour')\n",
    "        del t_df[f'{col}_at'], test_foruser[f'{col}_at']#, t_df[f'{col}_at_hour']\n",
    "    del f; gc.collect()\n",
    "    \n",
    "    # total days\n",
    "    t_df['total_days'] = (t_df['premium_registered_at_date'] - t_df['registered_at_date']).dt.days\n",
    "    test_foruser['total_days'] = (test_foruser['premium_registered_at_date'] - test_foruser['registered_at_date']).dt.days\n",
    "    del t_df['premium_registered_at_date'], t_df['registered_at_date']; gc.collect()\n",
    "    del test_foruser['premium_registered_at_date'], test_foruser['registered_at_date']; gc.collect()\n",
    "    \n",
    "    # birth_year features\n",
    "    for pct in [-5, -3, -2, -1, 1, 2, 3, 5]:\n",
    "        temp = meta_user.birth_year.value_counts().sort_index().pct_change(pct)\n",
    "        t_df[f'birth_year_pct_{pct}'] = t_df['birth_year'].map(temp)\n",
    "        test_foruser[f'birth_year_pct_{pct}'] = test_foruser['birth_year'].map(temp)\n",
    "    gc.collect()\n",
    "\n",
    "def account_pre(t_df, test_foruser, meta_account):\n",
    "    # uid account count, count discrepancy\n",
    "    group_df = meta_account.groupby('uid')\n",
    "    uid_account_count_min = group_df.count().min(axis=1)\n",
    "    uid_account_count_max = group_df.count().max(axis=1)\n",
    "    uid_account_count_dis = uid_account_count_max - uid_account_count_min\n",
    "    t_df['uid_account_count_min'] = t_df['uid'].map(uid_account_count_min)\n",
    "    t_df['uid_account_count_max'] = t_df['uid'].map(uid_account_count_max)\n",
    "    t_df['uid_account_count_dis'] = t_df['uid'].map(uid_account_count_dis)\n",
    "    test_foruser['uid_account_count_min'] = test_foruser['uid'].map(uid_account_count_min)\n",
    "    test_foruser['uid_account_count_max'] = test_foruser['uid'].map(uid_account_count_max)\n",
    "    test_foruser['uid_account_count_dis'] = test_foruser['uid'].map(uid_account_count_dis)\n",
    "    \n",
    "    # uid is_manual ratio\n",
    "    uid_is_manual_sum = group_df.is_manual.sum()\n",
    "    uid_is_manual_ratio = group_df.is_manual.sum() / group_df.is_manual.count()\n",
    "    t_df['uid_is_manual_sum'] = t_df['uid'].map(uid_is_manual_sum)\n",
    "    t_df['uid_is_manual_ratio'] = t_df['uid'].map(uid_is_manual_ratio)\n",
    "    test_foruser['uid_is_manual_sum'] = test_foruser['uid'].map(uid_is_manual_sum)\n",
    "    test_foruser['uid_is_manual_ratio'] = test_foruser['uid'].map(uid_is_manual_ratio)\n",
    "    \n",
    "    # uid unique service id count\n",
    "    uid_service_cat_id = group_df.service_category_id.unique().apply(lambda x: len(x))\n",
    "    t_df['uid_service_id_count'] = t_df['uid'].map(uid_service_cat_id)\n",
    "    test_foruser['uid_service_id_count'] = test_foruser['uid'].map(uid_service_cat_id)\n",
    "    \n",
    "    # uid service id when is_manual 1\n",
    "    temp = meta_account[meta_account.is_manual == 1].groupby('uid').service_category_id.unique().apply(lambda x: len(x))\n",
    "    t_df['uid_unique_service_id_when_manual'] = t_df['uid'].map(temp)\n",
    "    test_foruser['uid_unique_service_id_when_manual'] = test_foruser['uid'].map(temp)\n",
    "    temp = meta_account[meta_account.is_manual == 1].groupby('uid').service_category_id.unique().apply(lambda x: 13 in x).astype(np.int8)\n",
    "    t_df['uid_13_in_unique_service_id_when_manual'] = t_df['uid'].map(temp)\n",
    "    test_foruser['uid_13_in_unique_service_id_when_manual'] = test_foruser['uid'].map(temp)\n",
    "    \n",
    "    # uid service id when is_manual 0\n",
    "    temp = meta_account[meta_account.is_manual == 0].groupby('uid').service_category_id.unique().apply(lambda x: len(x))\n",
    "    t_df['uid_unique_service_id_when_not_manual'] = t_df['uid'].map(temp)\n",
    "    test_foruser['uid_unique_service_id_when_not_manual'] = test_foruser['uid'].map(temp)\n",
    "    temp = meta_account[meta_account.is_manual == 0].groupby('uid').service_category_id.apply(lambda x: stats.mode(x)[0]).astype(np.int8)\n",
    "    test_foruser['uid_mode_service_id_when_not_manual'] = test_foruser['uid'].map(temp)\n",
    "    \n",
    "    # uid most frequent service id\n",
    "    temp = group_df.service_category_id.apply(lambda x: stats.mode(x)[0]).astype(np.int8)\n",
    "    t_df['uid_mode_service_id'] = t_df['uid'].map(temp)\n",
    "    test_foruser['uid_mode_service_id'] = test_foruser['uid'].map(temp)\n",
    "    del temp\n",
    "    \n",
    "    # uid service id stat\n",
    "    for cat in meta_account.service_category_id.unique():\n",
    "        group_df = meta_account.groupby('uid').service_category_id\n",
    "        \n",
    "        #temp = meta_account.groupby('uid').service_category_id.apply(lambda x: list(x).count(cat))\n",
    "        #t_df[f'uid_service_id_{cat}_count'] = t_df['uid'].map(temp)\n",
    "        #test_foruser[f'uid_service_id_{cat}_count'] = test_foruser['uid'].map(temp)\n",
    "        \n",
    "        temp = meta_account.groupby('uid').service_category_id.apply(lambda x: list(x).count(cat))\n",
    "        count = meta_account.groupby('uid').service_category_id.count()\n",
    "        t_df[f'uid_service_id_{cat}_ratio'] = t_df['uid'].map(temp) / count.astype(np.int8)\n",
    "        test_foruser[f'uid_service_id_{cat}_ratio'] = test_foruser['uid'].map(temp) / count.astype(np.int8)\n",
    "\n",
    "    # uid date features\n",
    "    for col in ['created', 'first_succeeded']:\n",
    "        #f = lambda x: x.split(' ')[1]\n",
    "        #meta_account[f'{col}_at_hour'] = meta_account[f'{col}_at'].apply(f).astype(np.int8)\n",
    "        #apply_cyclical(t_df, f'{col}_at_hour')\n",
    "        \n",
    "        f = lambda x: x.split(' ')[0]\n",
    "        meta_account[f'{col}_at'] = pd.to_datetime(meta_account[f'{col}_at'].apply(f), format='%Y-%m-%d')\n",
    "        meta_account[f'{col}_at_day'] = meta_account[f'{col}_at'].dt.day#.astype(np.int8)\n",
    "        meta_account[f'{col}_at_month'] = meta_account[f'{col}_at'].dt.month#.astype(np.int8)\n",
    "        meta_account[f'{col}_at_year'] = meta_account[f'{col}_at'].dt.year#.astype(np.int16)\n",
    "        #apply_cyclical(meta_account, f'{col}_at_day')\n",
    "        #apply_cyclical(meta_account, f'{col}_at_month')\n",
    "        #apply_cyclical(meta_account, f'{col}_at_year')\n",
    "        \n",
    "        for shift in [-3, -2, -1, 1, 2, 3]:\n",
    "            group_df = meta_account.groupby('uid')[f'{col}_at']\n",
    "            meta_account[f'{col}_at_shift_{shift}'] = group_df.shift(shift)\n",
    "            meta_account[f'{col}_at_dis_{shift}'] = meta_account[f'{col}_at'] - meta_account[f'{col}_at_shift_{shift}']\n",
    "            \n",
    "            group_df = meta_account.groupby('uid')[f'{col}_at_dis_{shift}']\n",
    "            t_df[f'{col}_at_discrepancy_{shift}_min'] = t_df['uid'].map(group_df.min()).astype(np.int64)\n",
    "            test_foruser[f'{col}_at_discrepancy_{shift}_min'] = test_foruser['uid'].map(group_df.min()).astype(np.int64)\n",
    "\n",
    "        # created/first_suceeded date mode\n",
    "        for date_type in ['day', 'month', 'year']:\n",
    "            group_df = meta_account.groupby('uid')\n",
    "            temp = group_df[f'{col}_at_{date_type}'].apply(lambda x: x.mode())\n",
    "            t_df[f'uid_{col}_{date_type}_mode'] = t_df['uid'].map(temp)\n",
    "            test_foruser[f'uid_{col}_{date_type}_mode'] = test_foruser['uid'].map(temp)\n",
    "            \n",
    "            if date_type != 'day':\n",
    "                apply_target_encode(train_df, test_foruser, 'premium_expired', f'uid_{col}_{date_type}_mode', smooth=True, m=8)\n",
    "            apply_freq_encode(t_df, f'uid_{col}_{date_type}_mode')\n",
    "            apply_freq_encode(test_foruser, f'uid_{col}_{date_type}_mode')\n",
    "        \n",
    "            temp = group_df[f'{col}_at_{date_type}'].mean()\n",
    "            t_df[f'uid_{col}_{date_type}_mean'] = t_df['uid'].map(temp)\n",
    "            test_foruser[f'uid_{col}_{date_type}_mean'] = test_foruser['uid'].map(temp)\n",
    "            \n",
    "            temp = group_df[f'{col}_at_{date_type}'].max()\n",
    "            t_df[f'uid_{col}_{date_type}_max'] = t_df['uid'].map(temp)\n",
    "            test_foruser[f'uid_{col}_{date_type}_max'] = test_foruser['uid'].map(temp)\n",
    "            \n",
    "            temp = group_df[f'{col}_at_{date_type}'].min()\n",
    "            t_df[f'uid_{col}_{date_type}_min'] = t_df['uid'].map(temp)\n",
    "            test_foruser[f'uid_{col}_{date_type}_min'] = test_foruser['uid'].map(temp)\n",
    "            \n",
    "            temp = group_df[f'{col}_at_{date_type}'].std()\n",
    "            t_df[f'uid_{col}_{date_type}_std'] = t_df['uid'].map(temp)\n",
    "            test_foruser[f'uid_{col}_{date_type}_std'] = test_foruser['uid'].map(temp)\n",
    "        \n",
    "        temp = group_df[f'{col}_at'].max() - group_df[f'{col}_at'].min()\n",
    "        t_df[f'uid_{col}_span'] = t_df['uid'].map(temp).astype(np.int64)\n",
    "        test_foruser[f'uid_{col}_span'] = test_foruser['uid'].map(temp).astype(np.int64)\n",
    "        apply_freq_encode(t_df, f'uid_{col}_span')\n",
    "        apply_freq_encode(test_foruser, f'uid_{col}_span')\n",
    "        #apply_target_encode(train_df, test_foruser, 'premium_expired', f'uid_{col}_span', smooth=True, m=20)\n",
    "        \n",
    "        # is manual in past\n",
    "    del temp; gc.collect()\n",
    "\n",
    "def access_pre(t_df, test_foruser, meta_access):\n",
    "    group_df = meta_access.groupby('uid')\n",
    "    \n",
    "    # label encode\n",
    "    apply_label_encode(meta_access, ['channel_flag', 'url_type'])\n",
    "    \n",
    "    # uid channel flag mode\n",
    "    temp = group_df.channel_flag.apply(lambda x: x.mode())\n",
    "    t_df['uid_channel_flag_mode'] = t_df['uid'].map(temp)\n",
    "    test_foruser['uid_channel_flag_mode'] = test_foruser['uid'].map(temp)\n",
    "    \n",
    "    # uid url type unique\n",
    "    temp = group_df.url_type.unique().apply(lambda x: len(x))\n",
    "    t_df['uid_url_type_unique'] = t_df['uid'].map(temp)\n",
    "    test_foruser['uid_url_type_unique'] = test_foruser['uid'].map(temp)\n",
    "    \n",
    "    temp = group_df.url_type.apply(lambda x: stats.mode(x)[0][0])\n",
    "    t_df['uid_url_type_mode'] = t_df['uid'].map(temp)\n",
    "    test_foruser['uid_url_type_mode'] = test_foruser['uid'].map(temp)\n",
    "    \n",
    "    # access count stats\n",
    "    temp = group_df.count().max(axis=1)\n",
    "    t_df['uid_access_count'] = t_df['uid'].map(temp)\n",
    "    test_foruser['uid_access_count'] = test_foruser['uid'].map(temp)\n",
    "    temp = group_df.access_count.sum().astype(np.int32)\n",
    "    t_df['uid_real_access_count'] = t_df['uid'].map(temp)\n",
    "    test_foruser['uid_real_access_count'] = test_foruser['uid'].map(temp)\n",
    "    temp = group_df.access_count.std().astype(np.int32)\n",
    "    t_df['uid_access_count_std'] = t_df['uid'].map(temp)\n",
    "    test_foruser['uid_access_count_std'] = test_foruser['uid'].map(temp)\n",
    "    temp = group_df.access_count.min().astype(np.int32)\n",
    "    t_df['uid_access_count_min'] = t_df['uid'].map(temp)\n",
    "    test_foruser['uid_access_count_min'] = test_foruser['uid'].map(temp)\n",
    "    temp = group_df.access_count.mean().astype(np.int32)\n",
    "    t_df['uid_access_count_mean'] = t_df['uid'].map(temp)\n",
    "    test_foruser['uid_access_count_mean'] = test_foruser['uid'].map(temp)\n",
    "    \n",
    "    # freq encode and stat\n",
    "    apply_freq_encode(meta_access, 'channel_flag')\n",
    "    temp = meta_access.groupby('uid').channel_flag.max()\n",
    "    t_df['uid_mode_channel_flag_ratio'] = t_df['uid'].map(temp)\n",
    "    test_foruser['uid_mode_channel_flag_ratio'] = test_foruser['uid'].map(temp)\n",
    "    apply_freq_encode(meta_access, 'url_type')\n",
    "    temp = meta_access.groupby('uid').url_type.max()\n",
    "    t_df['uid_mode_url_type_ratio'] = t_df['uid'].map(temp)\n",
    "    test_foruser['uid_mode_url_type_ratio'] = test_foruser['uid'].map(temp)\n",
    "    \n",
    "    # access date stat\n",
    "    f = lambda x: x.split(' ')[1]\n",
    "    meta_access['accessed_at_hour'] = meta_access.accessed_at.apply(f).astype(np.int8)\n",
    "    \n",
    "    f = lambda x: x.split(' ')[0]\n",
    "    meta_access['accessed_at'] = pd.to_datetime(meta_access['accessed_at'].apply(f), format='%Y-%m-%d')\n",
    "    meta_access['accessed_day'] = meta_access['accessed_at'].dt.day.astype(np.int8)\n",
    "    meta_access['accessed_month'] = meta_access['accessed_at'].dt.month.astype(np.int8)\n",
    "    meta_access['accessed_year'] = meta_access['accessed_at'].dt.year.astype(np.int16)\n",
    "    #apply_cyclical(meta_account, f'{col}_at_day')\n",
    "    #apply_cyclical(meta_account, f'{col}_at_month')\n",
    "    #apply_cyclical(meta_account, f'{col}_at_year')\n",
    "    \n",
    "    # date modes\n",
    "    for date_type in ['day', 'month', 'year']:\n",
    "        group_df = meta_access.groupby('uid')[f'accessed_{date_type}']\n",
    "        \n",
    "        temp = group_df.apply(lambda x: x.mode()[0])\n",
    "        t_df[f'accessed_{date_type}_mode'] = t_df['uid'].map(temp)\n",
    "        test_foruser[f'accessed_{date_type}_mode'] = test_foruser['uid'].map(temp)\n",
    "        \n",
    "        temp = group_df.mean()\n",
    "        t_df[f'accessed_{date_type}_mean'] = t_df['uid'].map(temp)\n",
    "        test_foruser[f'accessed_{date_type}_mean'] = test_foruser['uid'].map(temp)\n",
    "        \n",
    "        temp = group_df.max()\n",
    "        t_df[f'accessed_{date_type}_max'] = t_df['uid'].map(temp)\n",
    "        test_foruser[f'accessed_{date_type}_max'] = test_foruser['uid'].map(temp)\n",
    "        \n",
    "        temp = group_df.min()\n",
    "        t_df[f'accessed_{date_type}_min'] = t_df['uid'].map(temp)\n",
    "        test_foruser[f'accessed_{date_type}_min'] = test_foruser['uid'].map(temp)\n",
    "    \n",
    "    for shift in [-3, -2, -1, 1, 2, 3]:\n",
    "        group_df = meta_access.groupby('uid')['accessed_at']\n",
    "        meta_access[f'accessed_at_shift_{shift}'] = group_df.shift(shift)\n",
    "        meta_access[f'accessed_at_dis_{shift}'] = meta_access['accessed_at'] - meta_access[f'accessed_at_shift_{shift}']\n",
    "    \n",
    "        group_df = meta_access.groupby('uid')[f'accessed_at_dis_{shift}']\n",
    "        t_df[f'accessed_at_discrepancy_{shift}_min'] = t_df['uid'].map(group_df.min()).astype(np.int64)\n",
    "        test_foruser[f'accessed_at_discrepancy_{shift}_min'] = test_foruser['uid'].map(group_df.min()).astype(np.int64)\n",
    "    del meta_access['accessed_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(meta_user, on='uid', how='left')\n",
    "train_df.loc[train_df.premium_expired == False, 'premium_expired'] = 0\n",
    "train_df.loc[train_df.premium_expired == True, 'premium_expired'] = 1\n",
    "test_foruser = test_foruser.merge(meta_user, on='uid', how='left')\n",
    "\n",
    "for col in ['gender', 'birth_year', 'family_structure']:\n",
    "    mean = train_df.groupby(col)['premium_expired'].mean()\n",
    "    std = train_df.groupby(col)['premium_expired'].std()\n",
    "    train_df[f'{col}_mean'] = train_df[col].map(mean)\n",
    "    train_df[f'{col}_std'] = train_df[col].map(std)\n",
    "    test_foruser[f'{col}_mean'] = test_foruser[col].map(mean)\n",
    "    test_foruser[f'{col}_std'] = test_foruser[col].map(std)\n",
    "    del mean, std; gc.collect()\n",
    "\n",
    "t_df_pre(train_df, test_foruser)\n",
    "gc.collect()\n",
    "account_pre(train_df, test_foruser, meta_account)\n",
    "gc.collect()\n",
    "access_pre(train_df, test_foruser, meta_access)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_kfold_clf(X_train, y_train,\n",
    "                  category_cols, metric='auc',\n",
    "                  learning_rate=0.07302765928886983, num_leaves=226.33302031283338, max_depth=89.28610428015179,\n",
    "                  bagging_fraction=0.8412111930704294, feature_fraction=0.2426023693040491,\n",
    "                  min_child_weight=0.11038748808743758, min_data_in_leaf=94.16721665585673,\n",
    "                  lambda_l1=2.780290034439586, lambda_l2=298.72604019749025,\n",
    "                  bayes_opt=True):\n",
    "    \n",
    "    params = {'objective': 'binary',\n",
    "              'metric': metric,\n",
    "              'boosting': 'gbdt',\n",
    "              'seed': 8982,\n",
    "              'learning_rate': learning_rate,\n",
    "              'num_leaves': int(num_leaves),\n",
    "              'max_depth': int(max_depth),\n",
    "              'bagging_freq': int(5),\n",
    "              'bagging_fraction': bagging_fraction,\n",
    "              'feature_fraction': feature_fraction,\n",
    "              'min_child_weight': min_child_weight,   \n",
    "              'min_data_in_leaf': int(min_data_in_leaf),\n",
    "              'lambda_l1': lambda_l1,\n",
    "              'lambda_l2': lambda_l2}\n",
    "              #'verbosity': int(-1)}\n",
    "             \n",
    "    import lightgbm as lgb\n",
    "    #cat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\n",
    "    #print(cat_features)\n",
    "    \n",
    "    oofs = np.zeros(X_train.shape[0])\n",
    "    models = []; learning_curves = []; best_scores = []; valid_score = []\n",
    "    \n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    kf = StratifiedKFold(n_splits=4, shuffle=True, random_state=8983)\n",
    "    for i, (train_idx, valid_idx) in enumerate(kf.split(X_train, y_train)):\n",
    "        X_train_fold = X_train.iloc[train_idx,:]\n",
    "        y_train_fold = y_train[train_idx]\n",
    "        X_valid_fold = X_train.iloc[valid_idx,:]\n",
    "        y_valid_fold = y_train[valid_idx]\n",
    "        d_train = lgb.Dataset(X_train_fold, label=y_train_fold, categorical_feature=category_cols)\n",
    "        d_valid = lgb.Dataset(X_valid_fold, label=y_valid_fold, categorical_feature=category_cols)\n",
    "        \n",
    "        learning_curve = {}\n",
    "        model = lgb.train(params,\n",
    "                          train_set=d_train,\n",
    "                          valid_sets=[d_train, d_valid],\n",
    "                          num_boost_round=600,\n",
    "                          evals_result=learning_curve,\n",
    "                          verbose_eval=200,#False,\n",
    "                          early_stopping_rounds=20)\n",
    "        best_score = {f'train_{metric}': model.best_score['training'][f'{metric}'],\n",
    "                      f'valid_{metric}': model.best_score['valid_1'][f'{metric}']}\n",
    "\n",
    "        oofs[valid_idx] = model.predict(X_train.iloc[valid_idx,:], num_iteration=model.best_iteration)\n",
    "        models.append(model)\n",
    "        learning_curves.append(learning_curve)\n",
    "        best_scores.append(best_score)\n",
    "        valid_score.append(best_score[f'valid_{metric}'])\n",
    "        del X_train_fold, y_train_fold, X_valid_fold, y_valid_fold, d_train, d_valid\n",
    "        gc.collect()\n",
    "        \n",
    "    valid_std_score = np.std(valid_score)\n",
    "    valid_avg_score = np.mean(valid_score)\n",
    "    \n",
    "    if bayes_opt:\n",
    "        return valid_avg_score\n",
    "    else:\n",
    "        return valid_avg_score, valid_std_score, models, oofs#best_scores, learning_curves\n",
    "\n",
    "def iterative_cv(X_train_full, y_train, added_cols):\n",
    "    init_valid_avg_score = lgb_kfold_clf(X_train_full[common_cols + category_cols], y_train, bayes_opt=True)\n",
    "    init_valid_avg_score *= -1\n",
    "    print(f'Current best score is {init_valid_avg_score}')\n",
    "    options = []; discards = []\n",
    "    for col in added_cols:\n",
    "        \n",
    "        X_train = X_train_full[common_cols + [col] + category_cols]\n",
    "        new_valid_avg_score = lgb_kfold_clf(X_train, y_train)\n",
    "        new_valid_avg_score *= -1\n",
    "        degree = new_valid_avg_score - init_valid_avg_score\n",
    "        if degree > 0:\n",
    "            options.append((col, degree))\n",
    "            print(f\"\\nFeature '{col}', improved CV score by {degree}\\n\")\n",
    "        else:\n",
    "            discards.append((col, degree))\n",
    "    options.sort(key=lambda tup: tup[1])\n",
    "    discards.sort(key=lambda tup: tup[1])\n",
    "    del col, X_train, degree; gc.collect()\n",
    "    return options, discards\n",
    "\n",
    "def iterative_elim(X_train_full, y_train, common_cols, category_cols):\n",
    "    options = []; discards = []\n",
    "    \n",
    "    init_valid_avg_score = lgb_kfold_clf(X_train_full[common_cols + category_cols], y_train, bayes_opt=True)\n",
    "    for col in common_cols:\n",
    "        temp_cols = list(set(common_cols)-{col}) + category_cols\n",
    "        X_train = X_train_full[temp_cols]\n",
    "        new_valid_avg_score = lgb_kfold_clf(X_train, y_train)\n",
    "        new_valid_avg_score *= 1\n",
    "        degree = new_valid_avg_score - init_valid_avg_score\n",
    "        if degree > 0: # degree < 0 if objective is to minimize metric.\n",
    "            options.append((col, degree))\n",
    "            print(f\"\\nFeature '{col}', improved CV score by {degree}\\n\")\n",
    "        else:\n",
    "            discards.append((col, degree))\n",
    "    options.sort(key=lambda tup: tup[1])\n",
    "    discards.sort(key=lambda tup: tup[1])\n",
    "    del col, X_train, degree; gc.collect()\n",
    "    return options, discards\n",
    "\n",
    "def f1_threshold_search(y_true, y_proba, linspace=100):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in [i * 0.01 for i in range(linspace)]:\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result\n",
    "\n",
    "def bayes_opt_lgbm(init_points=20, n_iteration=80):\n",
    "    from bayes_opt import BayesianOptimization\n",
    "    bounds = {'learning_rate': (0.001, 0.3),\n",
    "              'num_leaves': (20, 500), \n",
    "              'bagging_fraction' : (0.1, 1),\n",
    "              'feature_fraction' : (0.1, 1),\n",
    "              'min_child_weight': (0.001, 0.99),   \n",
    "              'min_data_in_leaf': (3, 200),\n",
    "              'max_depth': (-1, 100),\n",
    "              'lambda_l1': (0.1, 300), \n",
    "              'lambda_l2': (0.1, 300)}\n",
    "    optimizer = BayesianOptimization(f=lgb_kfold_clf, pbounds=bounds, random_state=8982)\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iteration)\n",
    "    \n",
    "    #print('Best score:', -optimizer.max['target'])\n",
    "    print('Best score:', optimizer.max['target'])\n",
    "    print('Best set of parameters:')\n",
    "    print(optimizer.max['params'])\n",
    "    param = optimizer.max['params']#; cv = -optimizer.max['target']\n",
    "    cv = optimizer.max['target']\n",
    "    return param, cv\n",
    "    \n",
    "def pred(X_test, models):\n",
    "    y_test_pred_total = np.zeros(X_test.shape[0])\n",
    "    for i, model in enumerate(models):\n",
    "        print(f'Predicting with {i}-th model')\n",
    "        y_pred_test = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "        y_test_pred_total += y_pred_test\n",
    "    y_test_pred_total /= len(models)\n",
    "    return y_test_pred_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_cols = ['uid', 'premium_expired']\n",
    "category_cols = ['gender',\n",
    "                 'birth_year',\n",
    "                 'family_structure',\n",
    "                 'premium_type']\n",
    "common_cols = list(set(train_df.columns) - set(del_cols) - set(category_cols))\n",
    "added_cols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = train_df[common_cols + category_cols]\n",
    "X_test_full = test_foruser[common_cols + category_cols]\n",
    "\n",
    "y_train = train_df['premium_expired'].values\n",
    "vas, vss, models, oofs = lgb_kfold_clf(X_train_full, y_train, category_cols, bayes_opt=False)\n",
    "print(f'valid_avg_score: {vas}')\n",
    "print(f'valid_std_score: {vss}')\n",
    "result = f1_threshold_search(y_train, oofs)\n",
    "print(result)\n",
    "print(f'X_train_full.shape: {X_train_full.shape}')\n",
    "print(f'X_test_full.shape: {X_test_full.shape}')\n",
    "\n",
    "feat_set = []\n",
    "for i in range(len(models)):\n",
    "    importance_df = pd.DataFrame(models[i].feature_importance(),\n",
    "                             index=[common_cols + category_cols],\n",
    "                             columns=['importance']).sort_values('importance')\n",
    "\n",
    "    feat_set.append([st[0] for st in importance_df[:80].index])\n",
    "\n",
    "unimportant = list(set(feat_set[0]).intersection(set(feat_set[1])).intersection(set(feat_set[2])).intersection(set(feat_set[3])))\n",
    "\n",
    "iterative_elim_cols = ['uid_created_day_min', 'uid_service_id_2_ratio',\n",
    "                       'birth_year_ratio', 'uid_created_day_std',\n",
    "                       'premium_registered_at_hour']\n",
    "\n",
    "common_cols = list(set(train_df.columns) - set(del_cols) - set(category_cols) - set(unimportant) - set(iterative_elim_cols))\n",
    "\n",
    "X_train_full = train_df[common_cols + category_cols]\n",
    "X_test_full = test_foruser[common_cols + category_cols]\n",
    "\n",
    "vas, vss, models, oofs = lgb_kfold_clf(X_train_full, y_train, category_cols, bayes_opt=False)\n",
    "print(f'valid_avg_score: {vas}')\n",
    "print(f'valid_std_score: {vss}')\n",
    "result = f1_threshold_search(y_train, oofs)\n",
    "print(result)\n",
    "print(f'X_train_full.shape: {X_train_full.shape}')\n",
    "print(f'X_test_full.shape: {X_test_full.shape}')\n",
    "y_test = pred(X_test_full, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oofs = pd.Series(oofs, name='premium_expired')\n",
    "#oofs.to_csv('XXX_OOF', index=False, header='premium_expired')#, float_format='%.4f')\n",
    "#sample_submission['premium_expired'] = y_test\n",
    "#sample_submission.to_csv('XXX_y_test_soft', index=False)\n",
    "#sample_submission[sample_submission.premium_expired <= result['threshold']] = int(0)\n",
    "#sample_submission[sample_submission.premium_expired > result['threshold']] = int(1)\n",
    "#sample_submission.to_csv('Han_ytest', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
