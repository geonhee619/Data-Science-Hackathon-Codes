{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env Related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce DataFrame Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "    from pandas.api.types import is_categorical_dtype\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            # skip datetime type or categorical type\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelize DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_parallelize_run(df, func):\n",
    "    import multiprocessing\n",
    "    num_partitions, num_cores = psutil.cpu_count(), psutil.cpu_count()\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = multiprocessing.Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Related (Categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_concat(df, subject_cols, print_option=True):\n",
    "    na_col = list(df.columns[df.isna().any()])\n",
    "    for col in na_col:\n",
    "        df[col].fillna('', inplace=True)\n",
    "    temp_str = ''\n",
    "    for col in subject_cols:\n",
    "        temp_str += '_' + col\n",
    "    df[temp_str[1:]] = ''\n",
    "    for col in subject_cols:\n",
    "        df[temp_str[1:]] += df[col]\n",
    "    \n",
    "    if print_option:\n",
    "        print(\"Generated features: category_concat\")\n",
    "        print(f\"'{temp_str[1:]}',\")\n",
    "        print()\n",
    "    del na_col, temp_str, col; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Encoder (OOF / smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class apply_target_encode:\n",
    "    \n",
    "    # (for train/train in oof)\n",
    "    # fit grouped label stats of given df\n",
    "    def fit_train(df, target_col, cat_col, m, statistic):\n",
    "        # df[target_col] = np.log1p(df[target_col])\n",
    "        df_group = df.groupby(cat_col)[target_col]\n",
    "        group_mean = df_group.mean().astype(np.float16)\n",
    "        temp_stat = []\n",
    "        # ===== smoothing =====\n",
    "        if m > 0:\n",
    "            global_mean = df[target_col].mean()\n",
    "            group_count = df_group.count().astype(np.float16)\n",
    "            smoother = ((group_count * group_mean) + (m * global_mean)) / (group_count + m)\n",
    "            temp_mean = (smoother, f'SMTH_MEAN_{m}')\n",
    "        # ===== no smoothing =====\n",
    "        elif m == 0:\n",
    "            temp_mean = (group_mean, 'MEAN')\n",
    "        # ===== more target statistic =====\n",
    "        if statistic:\n",
    "            group_min = df_group.min().astype(np.float16)\n",
    "            group_max = df_group.max().astype(np.float16)\n",
    "            group_std = df_group.std().astype(np.float16)\n",
    "            group_rng = group_max - group_min\n",
    "            group_Q1 = df_group.quantile(0.25).astype(np.float16)\n",
    "            group_Q2 = df_group.median().astype(np.float16)\n",
    "            group_Q3 = df_group.quantile(0.75).astype(np.float16)\n",
    "            group_IQR = group_Q3 - group_Q1\n",
    "            temp_stat = [(group_max, 'MAX'), (group_min, 'MIN'),\n",
    "                         (group_rng, 'RNG'), (group_std, 'STD'),\n",
    "                         (group_Q1, 'Q1'), (group_Q2, 'Q2'),\n",
    "                         (group_Q3, 'Q3'), (group_IQR, 'IQR')]\n",
    "        temp_stat.append(temp_mean)\n",
    "        return temp_stat\n",
    "    \n",
    "    # (for train/valid in oof)\n",
    "    # transform (encode) given df via given grouped label stats from fit_train\n",
    "    def transform_valid(temp_stat, df, valid_idx, cat_col, print_option):\n",
    "        for mapper, agg_str in temp_stat:\n",
    "            if fold == 0:\n",
    "                df[f'{cat_col}_{agg_str}'] = 'te_empty'\n",
    "            df.loc[valid_idx, f'{cat_col}_{agg_str}'] = df[cat_col].map(mapper)\n",
    "            if print_option:\n",
    "                print(f\"'{cat_col}_{agg_str}',\")\n",
    "                \n",
    "    # (for test in oof)\n",
    "    # fit_train and tranform_valid combined\n",
    "    def transform_test(df, test_df, target_col, cat_col, m, statistic, print_option):\n",
    "        temp_stat = apply_target_encode.fit_train(df, target_col, cat_col, m, statistic)\n",
    "        for mapper, agg_str in temp_stat:\n",
    "            test_df[f'{cat_col}_{agg_str}'] = test_df[f'{cat_col}'].map(mapper)\n",
    "            if print_option:\n",
    "                print(f\"'{cat_col}_{agg_str}',\")\n",
    "    \n",
    "    # (for ordinary use)\n",
    "    # fit_train and tranform_valid combined\n",
    "    def fit_transform(df, test_df, target_col, cat_col, m=0, statistic=False, print_option=True):\n",
    "        temp_stat = apply_target_encode.fit_train(df, target_col, cat_col, m, statistic)\n",
    "        for mapper, agg_str in temp_stat:\n",
    "            df[f'{cat_col}_{agg_str}'] = df[f'{cat_col}'].map(mapper)\n",
    "            test_df[f'{cat_col}_{agg_str}'] = test_df[f'{cat_col}'].map(mapper)\n",
    "            if print_option:\n",
    "                print(f\"'{cat_col}_{agg_str}',\")\n",
    "    \n",
    "    # train/train: fit grouped label statistic (with fit_train)\n",
    "    # train/valid: encode via the fitted (with transform_valid)\n",
    "    # test: fit with entire train, encode to test\n",
    "    # note: equal m/statistic is applied to all sets\n",
    "    def oof(df, test_df, target_col, cat_col, split, m=0, statistic=False, print_option=True):\n",
    "        # train/valid target encode\n",
    "        for fold, (train_idx, valid_idx) in enumerate(split):\n",
    "            temp_stat = apply_target_encode.fit_train(df=df.loc[train_idx, :],\n",
    "                                                      target_col=target_col, cat_col=cat_col,\n",
    "                                                      m=m, statistic=statistic)\n",
    "            apply_target_encode.transform_valid(temp_stat=temp_stat,\n",
    "                                                df=df, valid_idx=valid_idx, cat_col=cat_col,\n",
    "                                                print_option=False)\n",
    "            if 'te_empty' in df[f'{cat_col}_{agg_str}']:\n",
    "                print(f\"te_empty still left in '{cat_col}_{agg_str}'\")\n",
    "        # test oof (=train) target encode\n",
    "        apply_target_encode.transform_test(df=df, test_df=test_df,\n",
    "                                           target_col=target_col, cat_col=cat_col,\n",
    "                                           m=m, statistic=statistic,\n",
    "                                           print_option=print_option)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encode (not ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_label_encode(df, test_df, subject_cols):\n",
    "    lbl = LabelEncoder()\n",
    "    for str_col in subject_cols:\n",
    "        lbl.fit(df[str_col].unique())\n",
    "        df[str_col] = lbl.transform(df[str_col])\n",
    "        test_df[str_col] = lbl.transform(test_df[str_col])\n",
    "    del lbl, str_col; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (for meta)\n",
    "def apply_label_encode(df, subject_cols):\n",
    "    for str_col in subject_cols:\n",
    "        # ===== assumes Series of string =====\n",
    "        temp_dict = {value: i for i, value in enumerate(df[str_col].unique())}\n",
    "        df[str_col] = (df[str_col].map(temp_dict)).astype(np.int16)\n",
    "    del temp_dict, str_col; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency / Count Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_freq_encode(df, str_col, print_option=True):\n",
    "    temp_dict = {sample: df.loc[df[str_col]==sample].shape[0] for sample in df[str_col].unique()}\n",
    "    df[f'{str_col}_COUNT'] = df[str_col].map(temp_dict)\n",
    "    df[f'{str_col}_RATIO'] = df[str_col].map(temp_dict) / df[str_col].shape[0]\n",
    "    \n",
    "    if print_option:\n",
    "        print(f\"'{str_col}_COUNT',\")\n",
    "        print(f\"'{str_col}_RATIO',\")\n",
    "        print()\n",
    "    del temp_dict; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Related (Numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggs = {\n",
    "#     'uid': ['count'],\n",
    "#     'is_manual': ['sum', 'mean'],\n",
    "#     'elapsed_days_succeeded_created': ['mean', 'std', 'max', 'min'],\n",
    "#     'elapsed_days_created_premium': ['mean', 'std', 'max', 'min'],\n",
    "#     'elapsed_days_created': ['mean', 'std', 'max', 'min'],\n",
    "#     'elapsed_days_succeeded_premium': ['mean', 'std', 'max', 'min'],\n",
    "#     'elapsed_days_succeeded': ['mean', 'std', 'max', 'min'],\n",
    "#     'created_before_premium': ['sum', 'mean'],\n",
    "#     'created_after_premium': ['sum', 'mean'],\n",
    "#     'succeeded_before_premium': ['sum', 'mean'],\n",
    "#     'succeeded_before_premium': ['sum', 'mean'],\n",
    "# }\n",
    "# aggs.update({col: ['sum', 'mean'] for col in service_category_id_cols})\n",
    "\n",
    "# group_account_df = account_df.groupby(ID).agg(aggs)\n",
    "# group_account_df.columns = [f'{k}_{v.upper()}' for k, vs in aggs.items() for v in vs]\n",
    "# group_account_df = group_account_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cyclical Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cyclical(df, str_col):\n",
    "    # e.g. df['hr'] = df.timestamp.dt.hour; apply_cyclical(df, 'hr')\n",
    "    # ===== assumes integer array =====\n",
    "    # ===== assumes min and max exists in array =====\n",
    "    temp = pd.DataFrame()\n",
    "    temp['unique_sorted'] = (df[str_col] - df[str_col].min()).sort_values().unique()\n",
    "    int_max = temp.unique_sorted.max()\n",
    "    temp['sin'] = np.sin(2 * np.pi * temp.unique_sorted / int_max)\n",
    "    temp['cos'] = np.cos(2 * np.pi * temp.unique_sorted / int_max)\n",
    "    temp = temp.set_index('unique_sorted')\n",
    "    df[f'{str_col}_sin'] = (df[str_col] - df[str_col].min()).map(temp.sin)\n",
    "    df[f'{str_col}_cos'] = (df[str_col] - df[str_col].min()).map(temp.cos)\n",
    "    del temp, int_max; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mov_stat(df, str_col, list_windows, fix=False, print_option=True):\n",
    "\n",
    "    # ===== assumes timestamp is aligned =====\n",
    "    for win in list_windows:\n",
    "        rolled = df[str_col].rolling(window=win, min_periods=0)\n",
    "        mov_avg = rolled.mean().reset_index() #.astype(np.float16)\n",
    "        mov_max = rolled.max().reset_index() #.astype(np.float16)\n",
    "        mov_min = rolled.min().reset_index() #.astype(np.float16)\n",
    "        mov_std = rolled.std().reset_index() #.astype(np.float16)\n",
    "        if win >= 4:\n",
    "            mov_Q1 = rolled.quantile(0.25).reset_index() #.astype(np.float16)\n",
    "            mov_Q2 = rolled.quantile(0.5).reset_index() #.astype(np.float16)\n",
    "            mov_Q3 = rolled.quantile(0.75).reset_index() #.astype(np.float16)\n",
    "\n",
    "        if fix:\n",
    "            formula = int((win/2) - win)\n",
    "            df[f'{str_col}_movavg_{win}'] = mov_avg[f'{str_col}'].shift(formula)\n",
    "            df[f'{str_col}_movmax_{win}'] = mov_max[f'{str_col}'].shift(formula)\n",
    "            df[f'{str_col}_movmin_{win}'] = mov_min[f'{str_col}'].shift(formula)\n",
    "            df[f'{str_col}_movstd_{win}'] = mov_std[f'{str_col}'].shift(formula)\n",
    "            if win >= 4:\n",
    "                df[f'{str_col}_movQ1_{win}'] = mov_Q1[f'{str_col}'].shift(formula)\n",
    "                df[f'{str_col}_movQ2_{win}'] = mov_Q2[f'{str_col}'].shift(formula)\n",
    "                df[f'{str_col}_movQ3_{win}'] = mov_Q3[f'{str_col}'].shift(formula)\n",
    "            print()\n",
    "            del formula\n",
    "        else:\n",
    "            df[f'{str_col}_movavg_{win}'] = mov_avg[f'{str_col}']\n",
    "            df[f'{str_col}_movmax_{win}'] = mov_max[f'{str_col}']\n",
    "            df[f'{str_col}_movmin_{win}'] = mov_min[f'{str_col}']\n",
    "            df[f'{str_col}_movstd_{win}'] = mov_std[f'{str_col}']\n",
    "            if win >= 4:\n",
    "                df[f'{str_col}_movQ1_{win}'] = mov_Q1[f'{str_col}']\n",
    "                df[f'{str_col}_movQ2_{win}'] = mov_Q2[f'{str_col}']\n",
    "                df[f'{str_col}_movQ3_{win}'] = mov_Q3[f'{str_col}']\n",
    "            print()\n",
    "        \n",
    "        if print_option:\n",
    "            print('Generated features: apply_mov_stat')\n",
    "            print(f\"'{str_col}_movavg_{win}',\")\n",
    "            print(f\"'{str_col}_movmax_{win}',\")\n",
    "            print(f\"'{str_col}_movmin_{win}',\")\n",
    "            print(f\"'{str_col}_movstd_{win}',\")\n",
    "            if win >= 4:\n",
    "                print(f\"'{str_col}_movQ1_{win}',\")\n",
    "                print(f\"'{str_col}_movQ2_{win}',\")\n",
    "                print(f\"'{str_col}_movQ3_{win}',\")\n",
    "            print()\n",
    "            \n",
    "    del win, rolled, mov_avg, mov_max, mov_min, mov_std; gc.collect()\n",
    "    if any([val for val in list_windows if val >= 4]):\n",
    "        del mov_Q1, mov_Q2, mov_Q3; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_shift_feature(df, subject_cols, list_shift, print_option=True):\n",
    "    for col in subject_cols:\n",
    "        for step in list_shift:\n",
    "            df[f'{col}_shift_{step}'] = df[col].shift(int(step))\n",
    "            \n",
    "    if print_option:\n",
    "        for col in subject_cols:\n",
    "            for step in list_shift:\n",
    "                print(f\"'{col}_shift_{step}',\")\n",
    "        \n",
    "    del col, step; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oneth Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_oneth_feature(df, str_col, print_option=True):\n",
    "    import math\n",
    "    modify = np.vectorize(math.modf)\n",
    "    oneth, tenth = modify(df[str_col] / 10)\n",
    "    df[f'{str_col}_oneth'] = oneth * 10\n",
    "    \n",
    "    if print_option:\n",
    "        print(f\"'{str_col}_oneth',\")\n",
    "        \n",
    "    del tenth; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nan Binary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_isna_feature(df, subject_cols, print_option=True):\n",
    "    binary_isna = [col+\"_isnan\" for col in subject_cols]\n",
    "    df[binary_isna] = df[subject_cols].isna().astype(int)\n",
    "    \n",
    "    if print_option:\n",
    "        for col in binary_isna:\n",
    "            print(f\"'{col}',\")\n",
    "        \n",
    "    del binary_isna; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row nan Count Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_row_nan(df, print_option=True):\n",
    "    df['row_nan'] = df.isna().sum(axis=1).astype(np.int8)\n",
    "    \n",
    "    if print_option:\n",
    "        print(\"'row_nan',\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bruteforce Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bruteforce_combination(df, subject_cols, choose=2, print_option=True):\n",
    "    from itertools import combinations\n",
    "    comb = combinations(subject_cols, choose)\n",
    "    for feat_1, feat_2 in comb:\n",
    "        df[f'{feat_1}_.+_{feat_2}'] = df[f'{feat_1}'] + df[f'{feat_1}']\n",
    "        df[f'{feat_1}_.-_{feat_2}'] = df[f'{feat_1}'] - df[f'{feat_1}']\n",
    "        df[f'{feat_1}_.*_{feat_2}'] = df[f'{feat_1}'] * df[f'{feat_1}']\n",
    "        df[f'{feat_1}_./_{feat_2}'] = df[f'{feat_1}'] / df[f'{feat_1}']\n",
    "            \n",
    "    if print_option:\n",
    "        for feat_1, feat_2 in comb:\n",
    "            print(f\"'{feat_1}_.+_{feat_2}',\")\n",
    "            print(f\"'{feat_1}_.-_{feat_2}',\")\n",
    "            print(f\"'{feat_1}_.*_{feat_2}',\")\n",
    "            print(f\"'{feat_1}_./_{feat_2}',\")\n",
    "            \n",
    "    del comb, feat_1, feat_2; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>one</th>\n",
       "      <th>two</th>\n",
       "      <th>three</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   one  two  three\n",
       "0    1    2      3\n",
       "1    2    4      4\n",
       "2    3    7      4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x = pd.DataFrame(np.array([[1,2,3],[2,4,7],[3,4,4]]).transpose(), columns=['one', 'two', 'three']).astype(np.int8)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.,  2.,  3.,  6.,  6.],\n",
       "       [ 2.,  4.,  4.,  8.,  8., 16., 32.],\n",
       "       [ 3.,  7.,  4., 21., 12., 28., 84.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree=3, interaction_only=True, include_bias=False, order='C')\n",
    "poly.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x0', 'x1', 'x2', 'x0 x1', 'x0 x2', 'x1 x2', 'x0 x1 x2']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class apply_bin:\n",
    "    def cut(df, subject_cols, n_bin):\n",
    "        for col in subject_cols:\n",
    "            df[f'{col}_c_bin_{n_bin}'] = pd.cut(df[col], bins=n_bin, labels=False)\n",
    "    def qcut(df, subject_cols, n_q):\n",
    "        for col in subject_cols:\n",
    "            df[f'{col}_q_bin_{n_q}'] = pd.qcut(df[col], q=n_q, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gauss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.110594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.524689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-2.079909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.380613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.367914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gauss\n",
       "0  0.110594\n",
       "1  0.524689\n",
       "2 -2.079909\n",
       "3 -0.380613\n",
       "4 -0.367914"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bin = pd.DataFrame(np.random.randn(1000), columns=['gauss'])\n",
    "x_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gauss_c_bin_5\n",
      "2    507\n",
      "1    236\n",
      "3    220\n",
      "4     24\n",
      "0     13\n",
      "Name: gauss_c_bin_5, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "apply_bin.qcut(x_bin, ['gauss'], 5)\n",
    "apply_bin.cut(x_bin, ['gauss'], 5)\n",
    "print(x_bin.columns[-1])\n",
    "print(x_bin[x_bin.columns[-1]].value_counts())\n",
    "for i in range(x_bin[x_bin.columns[-1]].unique().shape[0]):\n",
    "    x_bin[x_bin[x_bin.columns[-1]] == i].gauss.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gauss_q_bin_5\n",
      "4    200\n",
      "3    200\n",
      "2    200\n",
      "1    200\n",
      "0    200\n",
      "Name: gauss_q_bin_5, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD4CAYAAADrRI2NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAARk0lEQVR4nO3de6xlZX3G8e8D0oJXNBztFJmOtkSGiIIdqYm9KKJFbVUabUsaS+plJEKqqU2kaJTWmtBUpTeijoE6WqtV8VYvVbzXpIqDooCHFmtREQJ4K6BWCvz6x17HHs51n+GsvfY+7/eT7Jy11l57ryeTmWfWWfvd70pVIUlqxwFDB5AkTZbFL0mNsfglqTEWvyQ1xuKXpMbcbegA4zjssMNqx44dQ8eQpJlyySWXfLuq5pZun4ni37FjB/v27Rs6hiTNlCRfX2m7l3okqTEWvyQ1xuKXpMZY/JLUGItfkhpj8UtSY3or/iQHJ7k4yZeSXJHkT7vtD0ryuSRXJfmnJD/VVwZJ0nJ9nvH/GDihqh4OHAuclORRwF8A51bVkcD3gGf3mEGStERvxV8jt3SrB3WPAk4A3tlt3ws8ra8MkqTlev3mbpIDgUuAXwDOA/4T+H5V3dbtcg1w+Cqv3Q3sBti+fXufMTWldpz5gTutX33OkwdKIm0tvX64W1W3V9WxwAOB44GdK+22ymv3VNWuqto1N7dsqglJ0n6ayKieqvo+8EngUcChSRZ+03ggcO0kMkiSRvoc1TOX5NBu+RDgRGAe+ATw9G63U4H39pVBkrRcn9f4twF7u+v8BwBvr6r3J/kK8LYkfw58ETi/xwySpCV6K/6q+jJw3Arbv8boer8kaQB+c1eSGmPxS1JjLH5JaozFL0mNsfglqTEWvyQ1xuKXpMZY/JLUGItfkhpj8UtSYyx+SWqMxS9JjbH4JakxFr8kNcbil6TGWPyS1BiLX5IaY/FLUmMsfklqjMUvSY2x+CWpMXcbOoCkyTjvtI/faf30150wUBINzTN+SWqMxS9JjbH4JakxvRV/kiOSfCLJfJIrkryg2352km8lubR7PKmvDJKk5fr8cPc24EVV9YUk9wIuSXJR99y5VfWqHo8tSVpFb8VfVdcB13XLNyeZBw7v63iSpPFM5Bp/kh3AccDnuk1nJPlykguS3HeV1+xOsi/JvhtvvHESMSWpCb0Xf5J7AhcCL6yqm4DXAj8PHMvoN4JXr/S6qtpTVbuqatfc3FzfMSWpGb0Wf5KDGJX+W6rqXQBVdX1V3V5VdwBvAI7vM4Mk6c76HNUT4Hxgvqpes2j7tkW7nQxc3lcGSdJyfY7qeTTwTOCyJJd2284CTklyLFDA1cDzeswgSVqiz1E9nwGywlMf7OuYkqT1+c1dSWqMxS9JjbH4JakxFr8kNcbil6TGWPyS1BiLX5IaY/FLUmMsfklqjMUvSY2x+CWpMRa/JDXG4pekxlj8ktQYi1+SGmPxS1JjLH5JaozFL0mNsfglqTEWvyQ1xuKXpMZY/JLUmLsNHUDt2nHmB+60fvU5Tx4oidQWz/glqTEWvyQ1xuKXpMb0VvxJjkjyiSTzSa5I8oJu+/2SXJTkqu7nffvKIElars8z/tuAF1XVTuBRwOlJjgbOBD5WVUcCH+vWJUkT0lvxV9V1VfWFbvlmYB44HHgqsLfbbS/wtL4ySJKWm8hwziQ7gOOAzwEPqKrrYPSfQ5L7r/Ka3cBugO3bt08ipga2dHin7przTvv40BE0pXr/cDfJPYELgRdW1U3jvq6q9lTVrqraNTc3119ASWpMr8Wf5CBGpf+WqnpXt/n6JNu657cBN/SZQZJ0Z32O6glwPjBfVa9Z9NT7gFO75VOB9/aVQZK0XJ/X+B8NPBO4LMml3bazgHOAtyd5NvAN4Bk9ZpAkLTFW8Sd5aFVdvpE3rqrPAFnl6cdt5L0kSZtn3Es9r0tycZLnJzm010SSpF6NdcZfVb+c5EjgWcC+JBcDf19VF/WaTjPPGTil6TP2h7tVdRXwUuDFwK8Bf5PkyiS/1Vc4SdLmG6v4kzwsybmMvn17AvCb3VQMJwDn9phPkrTJxh3V83fAG4CzqupHCxur6tokL+0lmSSpF+MW/5OAH1XV7QBJDgAOrqofVtWbe0snSdp0417j/yhwyKL1u3fbJEkzZtziP7iqbllY6Zbv3k8kSVKfxi3+HyR5xMJKkl8EfrTG/pKkKTXuNf4XAu9Icm23vg34nX4iSZL6NO4XuD6f5CjgIYymYbiyqv6312SSpF5sZJK2RwI7utccl4SqelMvqSRJvRl3krY3Az8PXArc3m0uwOKXpBkz7hn/LuDoqqo+w0iS+jdu8V8O/AxwXY9Z1ADvqzsblt6v9/TXnbCh5zXdxi3+w4CvdLNy/nhhY1U9pZdUkqTejFv8Z/cZQpI0OeMO5/xUkp8Djqyqjya5O3Bgv9EkSX0Yd1rm5wLvBF7fbToceE9foSRJ/Rl3yobTGd08/Sb4yU1Z7t9XKElSf8Yt/h9X1a0LK0nuxmgcvyRpxoz74e6nkpwFHJLk8cDzgX/uL5akSVo6PFNb27hn/GcCNwKXAc8DPsjo/ruSpBkz7qieOxjdevEN/caRJPVt3Ll6/osVrulX1YM3PZEkqVcbmatnwcHAM4D7rfWCJBcAvwHcUFUP7badDTyX0WUjGN28/YMbCSxJumvGusZfVd9Z9PhWVf0VsN7kHG8ETlph+7lVdWz3sPQlacLGvdTziEWrBzD6DeBea72mqj6dZMd+J5Mk9WLcSz2vXrR8G3A18Nv7ecwzkvw+sA94UVV9b6WdkuwGdgNs3759Pw8laTUO4WzXuKN6HrtJx3st8ApGHxS/gtF/KM9a5Zh7gD0Au3bt8stikrRJxr3U80drPV9Vrxnnfarq+kXv+Qbg/eO8TpK0eTYyqueRwPu69d8EPg18cyMHS7KtqhZu5nIyoxu8SJImaCM3YnlEVd0MPxmW+Y6qes5qL0jyVuAxwGFJrgFeDjwmybGMLvVczehbwJKkCRq3+LcDty5avxXYsdYLquqUFTafP+bxJEk9Gbf43wxcnOTdjM7WTwbe1FsqSVJvxh3V88okHwJ+pdv0B1X1xf5iaVZ5M/WtaSNDP70R+/Qbd3ZOgLsDN1XVXwPXJHlQT5kkST0a99aLLwdeDPxJt+kg4B/6CiVJ6s+4Z/wnA08BfgBQVdeyzpQNkqTpNG7x31pVRTc1c5J79BdJktSncYv/7UleDxya5LnAR/GmLJI0k8Yd1fOq7l67NwEPAV5WVRf1mkyS1It1iz/JgcCHq+pEwLKXNsH8UTvZeeX80DHUqHUv9VTV7cAPk9xnAnkkST0b95u7/wNcluQiupE9AFX1h72kkiT1Ztzi/0D3kCTNuDWLP8n2qvpGVe2dVCBJUr/Wu8b/noWFJBf2nEWSNAHrFX8WLT+4zyCSpMlY7xp/rbIsAc7GuRkc2qlJW6/4H57kJkZn/od0y3TrVVX37jWdJGnTrVn8VXXgpIJIkiZjI/PxS5K2AItfkhpj8UtSYyx+SWrMuFM2SLoLjtl7DJedetnQMXqzkZuxa3ie8UtSYyx+SWqMxS9Jjemt+JNckOSGJJcv2na/JBcluar7ed++ji9JWlmfZ/xvBE5asu1M4GNVdSTwsW5dkjRBvRV/VX0a+O6SzU8FFub23ws8ra/jS5JWNulr/A+oqusAup/3X23HJLuT7Euy78Ybb5xYQI1m3Fx4TLWzN/E20Ku81zF7j1nzZSs9v95rxjF/1M4Vl1dalzZqaj/crao9VbWrqnbNzc0NHUeStoxJF//1SbYBdD9vmPDxJal5ky7+9wGndsunAu+d8PElqXl9Dud8K/BvwEOSXJPk2cA5wOOTXAU8vluXJE1Qb3P1VNUpqzz1uL6OKUla39R+uCtJ6oezc2pNUz+kc7Gz7wNn//ed1+HO2zb6HqvY6rNtamvzjF+SGmPxS1JjLH5JaozFL0mNsfglqTEWvyQ1xuGcDVg6JPPqc5685vNb0uJhmmstr2Kzhm/OH7WTnVfOb8r7AJvyXpO29Mbsp7/uhIGStMszfklqjMUvSY2x+CWpMRa/JDXG4pekxlj8mh1n32f5yJufTMS2wfvvrrP/avfNPWbvMcueW1hfbftq5o/aueb9c1u51+55p338Jw9NhsUvSY2x+CWpMRa/JDXG4pekxlj8ktQYi1+SGuMkbQ3acpOybXQo5zjv96DtyzavNzxz3H1XGpa5MHnb/kzitvg1mzUJ3GZymOb08Yxfkhpj8UtSYyx+SWrMINf4k1wN3AzcDtxWVbuGyCFJLRryw93HVtW3Bzy+JDXJSz2S1JihzvgL+EiSAl5fVXuW7pBkN7AbYPv25UPr1LC7Mnxz8Wv3d2bPFaw1Q+fCvXrXm11ztWGe41rYdxqHdPZlrfv3em/f1Q11xv/oqnoE8ETg9CS/unSHqtpTVbuqatfc3NzkE0rSFjVI8VfVtd3PG4B3A8cPkUOSWjTx4k9yjyT3WlgGngBcPukcktSqIa7xPwB4d5KF4/9jVf3LADkkqUkTL/6q+hrw8EkfV5I04nBOSWqMs3NuAUtn27z6nCcPlGQKrDQ0c7Nn7+xsZLbOjVhrWOdqwzu3yo3Y7+oQTGcCHY9n/JLUGItfkhpj8UtSYyx+SWqMxS9JjbH4JakxDufcgrbczdQHstqMm/v7PrrrHK65OTzjl6TGWPyS1BiLX5IaY/FLUmMsfklqjMUvSY1xOGePNnPWTIdoboKeZukc0laZlXPWzPqN3D3jl6TGWPyS1BiLX5IaY/FLUmMsfklqjMUvSY3Z8sM5p/VG5A7PlNa3mbNxrjcEc1pm/pzEUFHP+CWpMRa/JDXG4pekxgxS/ElOSvLvSb6a5MwhMkhSqyZe/EkOBM4DnggcDZyS5OhJ55CkVg1xxn888NWq+lpV3Qq8DXjqADkkqUmpqskeMHk6cFJVPadbfybwS1V1xpL9dgO7u9WHAP8+0aArOwz49tAhNmDW8sLsZZ61vDB7mWctL0xP5p+rqrmlG4cYx58Vti3736eq9gB7+o8zviT7qmrX0DnGNWt5YfYyz1pemL3Ms5YXpj/zEJd6rgGOWLT+QODaAXJIUpOGKP7PA0cmeVCSnwJ+F3jfADkkqUkTv9RTVbclOQP4MHAgcEFVXTHpHPtpqi49jWHW8sLsZZ61vDB7mWctL0x55ol/uCtJGpbf3JWkxlj8ktQYi3+DkrwiyZeTXJrkI0l+duhMa0nyl0mu7DK/O8mhQ2daT5JnJLkiyR1JpnZI3KxNPZLkgiQ3JLl86CzjSHJEkk8kme/+Prxg6ExrSXJwkouTfKnL+6dDZ1qN1/g3KMm9q+qmbvkPgaOr6rSBY60qyROAj3cfqv8FQFW9eOBYa0qyE7gDeD3wx1W1b+BIy3RTj/wH8HhGQ5Q/D5xSVV8ZNNgakvwqcAvwpqp66NB51pNkG7Ctqr6Q5F7AJcDTpvXPOEmAe1TVLUkOAj4DvKCqPjtwtGU849+ghdLv3IMVvnw2TarqI1V1W7f6WUbfm5hqVTVfVdPwTe21zNzUI1X1aeC7Q+cYV1VdV1Vf6JZvBuaBw4dNtboauaVbPah7TGU/WPz7Ickrk3wT+D3gZUPn2YBnAR8aOsQWcTjwzUXr1zDFpTTrkuwAjgM+N2yStSU5MMmlwA3ARVU1lXkt/hUk+WiSy1d4PBWgql5SVUcAbwHOWPvd+rde3m6flwC3Mco8uHEyT7mxph7RXZfknsCFwAuX/MY9darq9qo6ltFv1scnmcpLalv+nrv7o6pOHHPXfwQ+ALy8xzjrWi9vklOB3wAeV1Pyoc4G/oynlVOPTEB3rfxC4C1V9a6h84yrqr6f5JPAScDUfZjuGf8GJTly0epTgCuHyjKOJCcBLwaeUlU/HDrPFuLUIz3rPiw9H5ivqtcMnWc9SeYWRs0lOQQ4kSntB0f1bFCSCxlNE30H8HXgtKr61rCpVpfkq8BPA9/pNn12mkchASQ5GfhbYA74PnBpVf36sKmWS/Ik4K/4/6lHXjlwpDUleSvwGEZTBl8PvLyqzh801BqS/DLwr8BljP69AZxVVR8cLtXqkjwM2Mvo78MBwNur6s+GTbUyi1+SGuOlHklqjMUvSY2x+CWpMRa/JDXG4pekxlj8ktQYi1+SGvN/oKrmPv8UC1UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(x_bin.columns[-2])\n",
    "print(x_bin[x_bin.columns[-2]].value_counts())\n",
    "for i in range(x_bin[x_bin.columns[-2]].unique().shape[0]):\n",
    "    x_bin[x_bin[x_bin.columns[-2]] == i].gauss.plot.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_clip(df, str_col, pct_lower, pct_upper):\n",
    "    LB, UB = np.percentile(df[str_col], [pct_lower, pct_upper])\n",
    "    df[str_col] = np.clip(df[str_col], LB, UB)\n",
    "    del LB, UB; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_interpolation(df, subject_cols, int_order, supp_median_fill=False):\n",
    "    lin = lambda var: var.interpolate(method='linear', limit_direction='both')\n",
    "    pol = lambda var: var.interpolate(method='polynomial', order=int_order, limit_direction='both')\n",
    "    \n",
    "    # ===== in ASHRAE, grouping was done via site_id =====\n",
    "    # linear = df.groupby(grouping_col).apply(lin)\n",
    "    # polyno = df.groupby(grouping_col).apply(pol)\n",
    "    \n",
    "    linear = df[subject_cols].apply(lin)\n",
    "    polyno = df[subject_cols].apply(pol)\n",
    "    df[subject_cols] = (linear[subject_cols] + polyno[subject_cols]) * 0.5\n",
    "    \n",
    "    # ===== if missing value remains: =====\n",
    "    if supp_median_fill:\n",
    "        #[col for col in cols if temp[col].isna().sum() > 0]\n",
    "        for col in subject_cols:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "            del col\n",
    "    del lin, pol, linear, polyno; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dist. Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import yeojohnson\n",
    "from scipy.stats import jarque_bera\n",
    "#from scipy.special import boxcox1p\n",
    "#from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "class apply_standardize:\n",
    "    def jarque_bera_test(df, str_col, how, alpha):\n",
    "        if any(alpha):\n",
    "            assert df[f'{str_col}_{how}'].shape[0] >= 2000, print('Not enough samples (>=2000)')\n",
    "            test_stat, p_val = jarque_bera(df[f'{str_col}_{how}'])\n",
    "            print(f'Jarque-Bera (Ï‡2) Normality Check:\\n- H0: is normally distributed\\n- H1: not from normal dist')\n",
    "            print(f'test-stat - {test_stat}\\np-value - {p_val}\\n')\n",
    "            for a in alpha:\n",
    "                if p_val < a:\n",
    "                    print(f'sig_lev: {a}\\nRejected 0 - nonnormal')\n",
    "                elif p_val > a:\n",
    "                    print(f\"sig_lev: {a}\\nRej Fail'd - normal\")\n",
    "\n",
    "    def internal_apply(df, str_col, how):\n",
    "        if how == 'Z':\n",
    "            sc = StandardScaler()\n",
    "            temp = df[str_col].values.reshape(df[str_col].shape[0], 1)\n",
    "            return sc.fit_transform(temp)\n",
    "        elif how == 'minmax':\n",
    "            ms = StandardScaler()\n",
    "            temp = df[str_col].values.reshape(df[str_col].shape[0], 1)\n",
    "            return ms.fit_transform(temp)\n",
    "        elif how == 'yeo':\n",
    "            yeo, yeo_lam = yeojohnson(df[str_col])\n",
    "            return yeo\n",
    "        else:\n",
    "            raise NotImplementedError(\"Acceptable arg how: 'Z', 'minmax', 'yeo'\")\n",
    "\n",
    "    def apply_ln(df, str_col):\n",
    "        temp_count = df[f'{str_col}'].isna().sum()\n",
    "        df[f'{str_col}'] = np.log1p(df[f'{str_col}'])\n",
    "        if df[f'{str_col}'].isna().sum() > temp_count:\n",
    "            print(f\"New nan in '{str_col}' via apply_nonlinear\")\n",
    "        del temp_count; gc.collect()\n",
    "\n",
    "    def apply(df, str_col, how, alpha=[], overwrite=True):\n",
    "        transformed = apply_standardize.internal_apply(df=df, str_col=str_col, how=how)\n",
    "        if overwrite:\n",
    "            df[f'{str_col}'] = transformed\n",
    "            apply_standardize.jarque_bera_test(df=df, str_col=str_col,\n",
    "                                             how=how, alpha=alpha)\n",
    "        elif not overwrite:\n",
    "            df[f'{str_col}_{how}'] = transformed\n",
    "            apply_standardize.jarque_bera_test(df=df, str_col=str_col,\n",
    "                                             how=how, alpha=alpha)\n",
    "            del df[f'{str_col}']; gc.collect()\n",
    "\n",
    "    def yeo_lambda():\n",
    "        None\n",
    "    def denormalize(lam_tup):\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gauss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.748642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.741789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.398563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.903010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.363437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gauss\n",
       "0  0.748642\n",
       "1  0.741789\n",
       "2  0.398563\n",
       "3  0.903010\n",
       "4  0.363437"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_box = pd.DataFrame(np.random.rand(10000), columns=['gauss'])\n",
    "x_box.gauss.plot.hist(bins=100)\n",
    "x_box.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jarque-Bera (Ï‡2) Normality Check:\n",
      "- H0: is normally distributed\n",
      "- H1: not from normal dist\n",
      "test-stat - 599.9084574751167\n",
      "p-value - 0.0\n",
      "\n",
      "sig_lev: 0.3\n",
      "Rejected 0 - nonnormal\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gauss_yeo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.679267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.673542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.376595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.806379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.344946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gauss_yeo\n",
       "0   0.679267\n",
       "1   0.673542\n",
       "2   0.376595\n",
       "3   0.806379\n",
       "4   0.344946"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_standardize.apply(x_box, 'gauss', how='yeo', alpha=[0.3], overwrite=False)\n",
    "x_box.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Validation with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_kfold_adv(X_train, X_test, category_cols, n_splits=5, shuffle=True):\n",
    "    X_train_adv = X_train.copy()\n",
    "    X_test_adv = X_test.copy()\n",
    "    X_train_adv['is_test'] = 0\n",
    "    X_test_adv['is_test'] = 1\n",
    "    X_adv = pd.concat([X_train_adv, X_test_adv], axis=0).reset_index(drop=True)\n",
    "    y_adv = X_adv['is_test'].values\n",
    "    del X_adv['is_test']; gc.collect()\n",
    "    \n",
    "    \n",
    "    split = build_cv_spliter(X_train=X_adv,\n",
    "                             y_train=y_adv,\n",
    "                             strategy='stratified',\n",
    "                             n_splits=n_splits,\n",
    "                             shuffle=shuffle,\n",
    "                             seed=8982)\n",
    "    \n",
    "    oofs, models, fimp = lgb_kfold_clf(X_train=X_adv,\n",
    "                                       y_train=y_adv,\n",
    "                                       category_cols=category_cols,\n",
    "                                       split=split,\n",
    "                                       bayes_opt=False)\n",
    "    \n",
    "    return fimp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.DataFrame(np.random.randn(200), columns=['gauss'])\n",
    "x_test = pd.DataFrame(np.random.randn(200)+3, columns=['gauss'])\n",
    "common_cols = list(set(x_train.columns).intersection(set(x_test.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== LightGBM Classifier training on : (400, 1) ==========\n",
      "========== LightGBM Classifier training: 1/5 fold ==========\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttraining's auc: 0.985469\tvalid_1's auc: 0.989062\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ome3\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== LightGBM Classifier training: 2/5 fold ==========\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\ttraining's auc: 0.98625\tvalid_1's auc: 0.98\n",
      "\n",
      "========== LightGBM Classifier training: 3/5 fold ==========\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\ttraining's auc: 0.983789\tvalid_1's auc: 0.985\n",
      "\n",
      "========== LightGBM Classifier training: 4/5 fold ==========\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's auc: 0.984141\tvalid_1's auc: 0.981563\n",
      "\n",
      "========== LightGBM Classifier training: 5/5 fold ==========\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\ttraining's auc: 0.985137\tvalid_1's auc: 0.970625\n",
      "\n",
      "====================\n",
      "CV AVG: auc - 0.98125\n",
      "CV STD: auc - 0.006158708265537508\n",
      "====================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>gauss</td>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>gauss</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>gauss</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>gauss</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>gauss</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature  importance  fold\n",
       "0   gauss         162     1\n",
       "0   gauss          42     2\n",
       "0   gauss          31     3\n",
       "0   gauss          47     4\n",
       "0   gauss          33     5"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_kfold_adv(x_train, x_test, category_cols=[], n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGroupKFold:\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        groups = pd.Series(groups)\n",
    "        unique_groups = np.unique(groups)\n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n",
    "        for tr_group_idx, va_group_idx in kf.split(unique_groups):\n",
    "            tr_groups, va_groups = unique_groups[tr_group_idx], unique_groups[va_group_idx]\n",
    "            tr_indices = groups[groups.isin(tr_groups)].index.to_list()\n",
    "            va_indices = groups[groups.isin(va_groups)].index.to_list()\n",
    "            yield tr_indices, va_indices\n",
    "            \n",
    "class StratifiedGroupKFold:\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    # Implementation based on this kaggle kernel:\n",
    "    #    https://www.kaggle.com/jakubwasikowski/stratified-group-k-fold-cross-validation\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        k = self.n_splits\n",
    "        rnd = check_random_state(self.random_state)\n",
    "            \n",
    "        # labels_num: zero-origin number of label\n",
    "        # ex) unique = [0,1,2,3] -> labels_num = 4\n",
    "        labels_num = np.max(y) + 1\n",
    "        \n",
    "        # y_counts_per_group: in-group label distribution\n",
    "        # y_distr: whole label distribution\n",
    "        y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n",
    "        y_distr = Counter()\n",
    "        for label, g in zip(y, groups):\n",
    "            y_counts_per_group[g][label] += 1\n",
    "            y_distr[label] += 1\n",
    "\n",
    "        # y_counts_per_fold: in-fold label distribution\n",
    "        y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n",
    "        groups_per_fold = defaultdict(set)\n",
    "        \n",
    "        # return mean std of per label counts when y_counts is in fold\n",
    "        def eval_y_counts_per_fold(y_counts, fold):\n",
    "            y_counts_per_fold[fold] += y_counts\n",
    "            std_per_label = []\n",
    "            for label in range(labels_num):\n",
    "                label_std = np.std(\n",
    "                    [y_counts_per_fold[i][label] / y_distr[label] for i in range(k)]\n",
    "                )\n",
    "                std_per_label.append(label_std)\n",
    "            y_counts_per_fold[fold] -= y_counts\n",
    "            return np.mean(std_per_label)\n",
    "        \n",
    "        # list of [group, y_counts]\n",
    "        # if shuffle: fold changes in same np.std(y_counts)\n",
    "        # ascending groups by degree of label variance\n",
    "        groups_and_y_counts = list(y_counts_per_group.items())\n",
    "        if self.shuffle:\n",
    "            rnd.shuffle(groups_and_y_counts)\n",
    "        groups_and_y_counts = sorted(groups_and_y_counts, key=lambda x: -np.std(x[1]))\n",
    "\n",
    "        # set fold for each group such that label distirbution will be uniform\n",
    "        for g, y_counts in groups_and_y_counts:\n",
    "            best_fold = None\n",
    "            min_eval = None\n",
    "            for i in range(k):\n",
    "                fold_eval = eval_y_counts_per_fold(y_counts, i)\n",
    "                if min_eval is None or fold_eval < min_eval:\n",
    "                    min_eval = fold_eval\n",
    "                    best_fold = i\n",
    "            y_counts_per_fold[best_fold] += y_counts\n",
    "            groups_per_fold[best_fold].add(g)\n",
    "\n",
    "        all_groups = set(groups)\n",
    "        for i in range(k):\n",
    "            train_groups = all_groups - groups_per_fold[i]\n",
    "            test_groups = groups_per_fold[i]\n",
    "\n",
    "            train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n",
    "            test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n",
    "\n",
    "            yield train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cv_spliter(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    strategy='stratified',\n",
    "    n_splits=5,\n",
    "    group=None,\n",
    "    shuffle=True,\n",
    "    seed=8982,\n",
    "    return_indices=False,\n",
    "):\n",
    "    if strategy == 'kfold':\n",
    "        kf = KFold(n_splits=n_splits, random_state=seed, shuffle=shuffle)\n",
    "        cv = kf.split(X_train)\n",
    "    elif strategy == 'stratified':\n",
    "        kf = StratifiedKFold(n_splits=n_splits, random_state=seed, shuffle=shuffle)\n",
    "        cv = kf.split(X_train, y_train)\n",
    "    elif strategy == 'group':\n",
    "        kf = MyGroupKFold(n_splits=n_splits, random_state=seed, shuffle=shuffle)\n",
    "        cv = kf.split(X_train, y_train, group)\n",
    "    elif strategy == 'stratified-group':\n",
    "        kf = StratifiedGroupKFold(n_splits=n_splits, random_state=seed, shuffle=shuffle)\n",
    "        cv = kf.split(X_train, y_train, group)\n",
    "    else:\n",
    "        raise NotImplementedError(f'strategy {strategy} not implemented.')\n",
    "\n",
    "    if not return_indices:\n",
    "        cv_spliter = []\n",
    "        for dev_idx, val_idx in cv:\n",
    "            cv_spliter.append([dev_idx, val_idx])\n",
    "        return cv_spliter\n",
    "    else:\n",
    "        fold_indices = np.zeros(len(X), dtype=np.int64)\n",
    "        for fold, (_, val_idx) in enumerate(cv):\n",
    "            fold_indices[val_idx] = int(fold)\n",
    "        return fold_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM (Binary Classification: max auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "# from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_kfold_clf(X_train, y_train, category_cols, split, bayes_opt=True,\n",
    "                  learning_rate=0.05, num_leaves=31, max_depth=-1,\n",
    "                  bagging_fraction=0.9, feature_fraction=0.9,\n",
    "                  min_child_weight=1e-3, min_data_in_leaf=20,\n",
    "                  lambda_l1=0.0, lambda_l2=0.0):\n",
    "    metric='auc'\n",
    "    params = {'objective': 'binary',\n",
    "              'metric': metric,\n",
    "              'boosting': 'gbdt',\n",
    "              'seed': 8982,\n",
    "              'learning_rate': learning_rate,\n",
    "              'num_leaves': int(num_leaves),\n",
    "              'max_depth': int(max_depth),\n",
    "              'bagging_freq': int(5),\n",
    "              'bagging_fraction': bagging_fraction,\n",
    "              'feature_fraction': feature_fraction,\n",
    "              'min_child_weight': min_child_weight,   \n",
    "              'min_data_in_leaf': int(min_data_in_leaf),\n",
    "              'lambda_l1': lambda_l1,\n",
    "              'lambda_l2': lambda_l2}\n",
    "              #'verbosity': int(-1)}\n",
    "             \n",
    "    #cat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\n",
    "    #print(cat_features)\n",
    "    \n",
    "    n_splits = len(split)\n",
    "    oofs = np.zeros(X_train.shape[0])\n",
    "    models = []; learning_curves = []; best_scores = []; valid_score = []\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "    print(f'========== LightGBM Classifier training on : {X_train.shape} ==========')\n",
    "    for i, (train_idx, valid_idx) in enumerate(split):\n",
    "        d_train = lgb.Dataset(X_train.iloc[train_idx,:], label=y_train[train_idx], categorical_feature=category_cols)\n",
    "        d_valid = lgb.Dataset(X_train.iloc[valid_idx,:], label=y_train[valid_idx], categorical_feature=category_cols)\n",
    "        \n",
    "        print(f'========== LightGBM Classifier training: {i+1}/{n_splits} fold ==========')\n",
    "        learning_curve = {}\n",
    "        model = lgb.train(params,\n",
    "                          train_set=d_train,\n",
    "                          valid_sets=[d_train, d_valid],\n",
    "                          num_boost_round=5000,\n",
    "                          early_stopping_rounds=20,\n",
    "                          evals_result=learning_curve,\n",
    "                          verbose_eval=200#False,\n",
    "                          )\n",
    "        best_score = {f'train_{metric}': model.best_score['training'][f'{metric}'],\n",
    "                      f'valid_{metric}': model.best_score['valid_1'][f'{metric}']}\n",
    "        print()\n",
    "        oofs[valid_idx] = model.predict(X_train.iloc[valid_idx,:], num_iteration=model.best_iteration)\n",
    "        models.append(model)\n",
    "        learning_curves.append(learning_curve)\n",
    "        valid_score.append(best_score[f'valid_{metric}'])\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = common_cols + category_cols\n",
    "        fold_importance_df['importance'] = model.feature_importance()\n",
    "        fold_importance_df['fold'] = i+1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        \n",
    "        del d_train, d_valid, fold_importance_df\n",
    "        gc.collect()\n",
    "        \n",
    "    valid_std_score = np.std(valid_score)\n",
    "    valid_avg_score = np.mean(valid_score)\n",
    "    print('====================')\n",
    "    print(f'CV AVG: {metric} - {valid_avg_score}')\n",
    "    print(f'CV STD: {metric} - {valid_std_score}')\n",
    "    print('====================')\n",
    "\n",
    "    if bayes_opt:\n",
    "        return valid_avg_score\n",
    "    else:\n",
    "        return oofs, models, feature_importance_df #best_scores, learning_curves\n",
    "\n",
    "def lgb_clf_bayes_opt(init_points=20, n_iteration=80):\n",
    "    bounds = {'learning_rate': (0.001, 0.3),\n",
    "              'num_leaves': (20, 500), \n",
    "              #'max_depth': (-1, 250),\n",
    "              'bagging_fraction' : (0.1, 1),\n",
    "              'feature_fraction' : (0.1, 1),\n",
    "              'min_child_weight': (0.001, 0.99),   \n",
    "              'min_data_in_leaf': (3, 700),\n",
    "              'lambda_l1': (0.1, 300), \n",
    "              'lambda_l2': (0.1, 300)}\n",
    "    \n",
    "    optimizer = BayesianOptimization(f=lgb_kfold_clf, pbounds=bounds, random_state=8982)\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iteration)\n",
    "    \n",
    "    print('Best score:', optimizer.max['target'])\n",
    "    print('Best set of parameters:')\n",
    "    print(optimizer.max['params'])\n",
    "    param = optimizer.max['params']; cv = optimizer.max['target']\n",
    "    return param, cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM (Regression: min rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "# from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_kfold_reg(X_train, y_train, category_cols, split, bayes_opt=True,\n",
    "                  learning_rate=0.05, num_leaves=31, max_depth=-1,\n",
    "                  bagging_fraction=0.9, feature_fraction=0.9,\n",
    "                  min_child_weight=1e-3, min_data_in_leaf=20,\n",
    "                  lambda_l1=0.0, lambda_l2=0.0):\n",
    "    metric='rmse'\n",
    "    params = {'objective': 'regression',\n",
    "              'metric': metric,\n",
    "              'boosting': 'gbdt',\n",
    "              'seed': 8982,\n",
    "              'learning_rate': learning_rate,\n",
    "              'num_leaves': int(num_leaves),\n",
    "              'max_depth': int(max_depth),\n",
    "              'bagging_freq': int(5),\n",
    "              'bagging_fraction': bagging_fraction,\n",
    "              'feature_fraction': feature_fraction,\n",
    "              'min_child_weight': min_child_weight,   \n",
    "              'min_data_in_leaf': int(min_data_in_leaf),\n",
    "              'lambda_l1': lambda_l1,\n",
    "              'lambda_l2': lambda_l2}\n",
    "              #'verbosity': int(-1)}\n",
    "             \n",
    "    #cat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\n",
    "    #print(cat_features)\n",
    "    \n",
    "    n_splits = len(split)\n",
    "    oofs = np.zeros(X_train.shape[0])\n",
    "    models = []; learning_curves = []; valid_scores = []\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "    print(f'========== LightGBM Regressor training on: {X_train.shape} ==========')\n",
    "    for i, (train_idx, valid_idx) in enumerate(split):\n",
    "        d_train = lgb.Dataset(X_train.iloc[train_idx,:], label=y_train[train_idx], categorical_feature=category_cols)\n",
    "        d_valid = lgb.Dataset(X_train.iloc[valid_idx,:], label=y_train[valid_idx], categorical_feature=category_cols)\n",
    "        \n",
    "        print(f'========== LightGBM Regressor training: {i+1}/{n_splits} fold ==========')\n",
    "        learning_curve = {}\n",
    "        model = lgb.train(params,\n",
    "                          train_set=d_train,\n",
    "                          valid_sets=[d_train, d_valid],\n",
    "                          num_boost_round=5000,\n",
    "                          early_stopping_rounds=20,\n",
    "                          evals_result=learning_curve,\n",
    "                          verbose_eval=200#False,\n",
    "                          )\n",
    "        print()\n",
    "        oofs[valid_idx] = model.predict(X_train.iloc[valid_idx,:], num_iteration=model.best_iteration)\n",
    "        models.append(model)\n",
    "        learning_curves.append(learning_curve)\n",
    "        valid_scores.append(model.best_score['valid_1'][f'{metric}'])\n",
    "          \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = common_cols + category_cols\n",
    "        fold_importance_df['importance'] = model.feature_importance()\n",
    "        fold_importance_df['fold'] = i+1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)  \n",
    "        \n",
    "        del d_train, d_valid, fold_importance_df\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    valid_std_score = np.std(valid_scores)\n",
    "    valid_avg_score = np.mean(valid_scores)\n",
    "    print('====================')\n",
    "    print(f'CV AVG: {metric} - {valid_avg_score}')\n",
    "    print(f'CV STD: {metric} - {valid_std_score}')\n",
    "    print('====================')\n",
    "    \n",
    "    if bayes_opt:\n",
    "        return -valid_avg_score\n",
    "    else:\n",
    "        return oofs, models, feature_importance_df #best_scores, learning_curves\n",
    "\n",
    "def lgb_reg_bayes_opt(init_points=20, n_iteration=80):\n",
    "    bounds = {'learning_rate': (0.001, 0.3),\n",
    "              'num_leaves': (20, 500),\n",
    "              #'max_depth': (-1, 250),\n",
    "              'bagging_fraction' : (0.1, 1),\n",
    "              'feature_fraction' : (0.1, 1),\n",
    "              'min_child_weight': (0.001, 0.99),   \n",
    "              'min_data_in_leaf': (3, 700),\n",
    "              'lambda_l1': (0.1, 300), \n",
    "              'lambda_l2': (0.1, 300)}\n",
    "    \n",
    "    optimizer = BayesianOptimization(f=lgb_kfold_reg, pbounds=bounds, random_state=8982)\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iteration)\n",
    "    \n",
    "    print('Best score:', -optimizer.max['target'])\n",
    "    print('Best set of parameters:')\n",
    "    print(optimizer.max['params'])\n",
    "    param = optimizer.max['params']; cv = -optimizer.max['target']\n",
    "    return param, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_pred(X_test, models):\n",
    "    y_test_pred_total = np.zeros(X_test.shape[0])\n",
    "    for i, model in enumerate(models):\n",
    "        print(f'========== LightGBM Predicting with {i+1}-th model ==========')\n",
    "        y_pred_test = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "        y_test_pred_total += y_pred_test\n",
    "    y_test_pred_total /= len(models)\n",
    "    return y_test_pred_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost (Binary Classification: max AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_kfold_clf(X_train, y_train, category_cols, split, bayes_opt=True,\n",
    "                learning_rate=0.03, num_leaves=31, max_depth=6,\n",
    "                subsample=0.8, bagging_temperature=1.0, colsample_bylevel=1.0,\n",
    "                min_data_in_leaf=1, l2_leaf_reg=3.0, random_strength=1.0):\n",
    "    loss = 'Logloss'\n",
    "    metric = 'AUC'\n",
    "    params = {'loss_function': loss,\n",
    "              'eval_metric': metric,\n",
    "              'boosting_type': 'Plain',\n",
    "              'random_seed': 8982,\n",
    "              'num_boost_round': 5000,\n",
    "              'early_stopping_rounds': 20,\n",
    "              'use_best_model': True,\n",
    "              # 'grow_policy': 'SymmetricTree','Depthwise','Lossguide',\n",
    "              'nan_mode': 'Max',\n",
    "              'od_type': 'Iter',\n",
    "              'verbose': 200,\n",
    "              \n",
    "              'learning_rate': learning_rate,\n",
    "              'num_leaves': int(num_leaves),\n",
    "              'max_depth': int(max_depth),\n",
    "              'subsample': subsample, #bf?\n",
    "              'bagging_temperature': bagging_temperature, #bf?\n",
    "              'colsample_bylevel': colsample_bylevel, #ff\n",
    "              'min_data_in_leaf': int(min_data_in_leaf),\n",
    "              'l2_leaf_reg': l2_leaf_reg,\n",
    "              'random_strength': random_strength}\n",
    "    \n",
    "    n_splits = len(split)\n",
    "    oofs = np.zeros(X_train.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "    models = []; learning_curves = []; valid_losses = []; valid_metrics = []\n",
    "    print(f'========== CatBoost Classifier training on: {X_train.shape} ==========')\n",
    "    for i, (train_idx, valid_idx) in enumerate(split):\n",
    "        print(f'========== CatBoost Classifier training: {i+1}/{n_splits} ==========')\n",
    "        train_d = cb.Pool(data=X_train.loc[train_idx],\n",
    "                          label=y_train[train_idx],\n",
    "                          cat_features=category_cols)\n",
    "        valid_d = cb.Pool(data=X_train.loc[valid_idx],\n",
    "                          label=y_train[valid_idx],\n",
    "                          cat_features=category_cols)\n",
    "        \n",
    "        model = cb.CatBoostClassifier(**params)\n",
    "        model.fit(train_d, eval_set=valid_d)\n",
    "        \n",
    "        oofs[valid_idx] = model.predict_proba(X_train.loc[valid_idx])[:,1]\n",
    "        models.append(model)\n",
    "        valid_losses.append(model.best_score_['validation'][f'{loss}'])\n",
    "        valid_metrics.append(model.best_score_['validation'][f'{metric}'])\n",
    "          \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = common_cols + category_cols\n",
    "        fold_importance_df['importance'] = model.get_feature_importance()\n",
    "        fold_importance_df['fold'] = i+1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "          \n",
    "        del train_d, valid_d, model, fold_importance_df\n",
    "        gc.collect()\n",
    "        \n",
    "    print('====================')\n",
    "    print(f'CV AVG:\\n{loss} - {np.mean(valid_losses)}\\n{metric} - {np.mean(valid_metrics)}')\n",
    "    print(f'CV STD:\\n{loss} - {np.std(valid_losses)}\\n{metric} - {np.std(valid_metrics)}')\n",
    "    print('====================')\n",
    "    \n",
    "    if bayes_opt:\n",
    "        return np.mean(valid_metrics)\n",
    "    else:\n",
    "        return oofs, models, feature_importance_df\n",
    "\n",
    "def cb_clf_bayes_opt(init_points=20, n_iteration=80):\n",
    "    bounds = {'learning_rate': (0.001, 0.3),\n",
    "              'num_leaves': (16, 288), \n",
    "              'max_depth': (3, 16),\n",
    "              'subsample' : (0.1, 1),\n",
    "              'bagging_temperature' : (0, 100),\n",
    "              'colsample_bylevel': (0.001, 1),   \n",
    "              'min_data_in_leaf': (3, 700),\n",
    "              'l2_leaf_reg': (0.1, 300), \n",
    "              'random_strength': (0, 100)}\n",
    "    \n",
    "    optimizer = BayesianOptimization(f=cb_kfold_clf, pbounds=bounds, random_state=8982)\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iteration)\n",
    "    \n",
    "    print('Best score:', optimizer.max['target'])\n",
    "    print('Best set of parameters:')\n",
    "    print(optimizer.max['params'])\n",
    "    param = optimizer.max['params']; cv = optimizer.max['target']\n",
    "    return param, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_pred_clf(X_test, models):\n",
    "    y_test_pred_total = np.zeros(X_test.shape[0])\n",
    "    for i, model in enumerate(models):\n",
    "        print(f'========== CatBoost Predicting with {i+1}-th model ==========')\n",
    "        y_pred_test = model.predict_proba(X_test)[:,1]\n",
    "        y_test_pred_total += y_pred_test\n",
    "    y_test_pred_total /= len(models)\n",
    "    return y_test_pred_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost (Regression: min RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_kfold_reg(X_train, y_train, category_cols, split, bayes_opt=True,\n",
    "                learning_rate=0.03, num_leaves=31, max_depth=6,\n",
    "                subsample=0.8, bagging_temperature=1.0, colsample_bylevel=1.0,\n",
    "                min_data_in_leaf=1, l2_leaf_reg=3.0, random_strength=1.0):\n",
    "    loss = 'RMSE'\n",
    "    metric = 'RMSE'\n",
    "    params = {'loss_function': loss,\n",
    "              'eval_metric': metric,\n",
    "              'boosting_type': 'Plain',\n",
    "              'random_seed': 8982,\n",
    "              'num_boost_round': 5000,\n",
    "              'early_stopping_rounds': 20,\n",
    "              'use_best_model': True,\n",
    "              # 'grow_policy': 'SymmetricTree','Depthwise','Lossguide',\n",
    "              'nan_mode': 'Max',\n",
    "              'od_type': 'Iter',\n",
    "              'verbose': 200,\n",
    "              \n",
    "              'learning_rate': learning_rate,\n",
    "              'num_leaves': int(num_leaves),\n",
    "              'max_depth': int(max_depth),\n",
    "              'subsample': subsample, #bf?\n",
    "              'bagging_temperature': bagging_temperature, #bf?\n",
    "              'colsample_bylevel': colsample_bylevel, #ff\n",
    "              'min_data_in_leaf': int(min_data_in_leaf),\n",
    "              'l2_leaf_reg': l2_leaf_reg,\n",
    "              'random_strength': random_strength}\n",
    "    \n",
    "    n_splits = len(split)\n",
    "    oofs = np.zeros(X_train.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "    models = []; valid_losses = []; valid_metrics = []\n",
    "    print(f'========== CatBoost Regressor training on: {X_train.shape} ==========')\n",
    "    for i, (train_idx, valid_idx) in enumerate(split):\n",
    "        print(f'========== CatBoost Regressor training: {i+1}/{n_splits} ==========')\n",
    "        train_d = cb.Pool(data=X_train.loc[train_idx],\n",
    "                          label=y_train[train_idx],\n",
    "                          cat_features=category_cols)\n",
    "        valid_d = cb.Pool(data=X_train.loc[valid_idx],\n",
    "                          label=y_train[valid_idx],\n",
    "                          cat_features=category_cols)\n",
    "        \n",
    "        model = cb.CatBoostRegressor(**params)\n",
    "        model.fit(train_d, eval_set=valid_d)\n",
    "        \n",
    "        oofs[valid_idx] = model.predict(X_train.loc[valid_idx])\n",
    "        models.append(model)\n",
    "        # valid_losses.append(model.best_score_['validation'][f'{loss}'])\n",
    "        valid_metrics.append(model.best_score_['validation'][f'{metric}'])\n",
    "          \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = common_cols + category_cols\n",
    "        fold_importance_df['importance'] = model.get_feature_importance()\n",
    "        fold_importance_df['fold'] = i+1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "          \n",
    "        del train_d, valid_d, model, fold_importance_df\n",
    "        gc.collect()\n",
    "    \n",
    "    assert loss == metric\n",
    "    print('====================')\n",
    "    print(f'CV AVG: {metric} - {np.mean(valid_metrics)}')\n",
    "    print(f'CV STD: {metric} - {np.std(valid_metrics)}')\n",
    "    print('====================')\n",
    "    \n",
    "    if bayes_opt:\n",
    "        return -np.mean(valid_metrics)\n",
    "    else:\n",
    "        return oofs, models, feature_importance_df\n",
    "\n",
    "def cb_reg_bayes_opt(init_points=20, n_iteration=80):\n",
    "    bounds = {'learning_rate': (0.001, 0.3),\n",
    "              'num_leaves': (16, 288), \n",
    "              'max_depth': (3, 16),\n",
    "              'subsample' : (0.1, 1),\n",
    "              'bagging_temperature' : (0, 100),\n",
    "              'colsample_bylevel': (0.001, 1),   \n",
    "              'min_data_in_leaf': (3, 700),\n",
    "              'l2_leaf_reg': (0.1, 300), \n",
    "              'random_strength': (0, 100)}\n",
    "    \n",
    "    optimizer = BayesianOptimization(f=cb_kfold_clf, pbounds=bounds, random_state=8982)\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iteration)\n",
    "    \n",
    "    print('Best score:', -optimizer.max['target'])\n",
    "    print('Best set of parameters:')\n",
    "    print(optimizer.max['params'])\n",
    "    param = optimizer.max['params']; cv = -optimizer.max['target']\n",
    "    return param, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_pred_reg(X_test, models):\n",
    "    y_test_pred_total = np.zeros(X_test.shape[0])\n",
    "    for i, model in enumerate(models):\n",
    "        print(f'========== CatBoost Predicting with {i+1}-th model ==========')\n",
    "        y_pred_test = model.predict(X_test)#[:,1]\n",
    "        y_test_pred_total += y_pred_test\n",
    "    y_test_pred_total /= len(models)\n",
    "    return y_test_pred_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is there no need to set cb.predict(, best_iteration=?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (Binary Classification: max auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# encode categorical cols beforehand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_kfold_clf(X_train, y_train, split, bayes_opt=True,\n",
    "                  eta=0.3, gamma=0, max_depth=6, min_child_weight=1,\n",
    "                  subsample=0.7, colsample_bytree=1.0, colsample_bylevel=1.0,\n",
    "                  colsample_bynode=1.0, reg_lambda=1.0, reg_alpha=0.0):\n",
    "    metric = 'auc'\n",
    "    params = {'objective': 'binary:logistic',\n",
    "              'eval_metric': metric,\n",
    "              'booster': 'gbtree', # 'dart',\n",
    "              'seed': 8982,\n",
    "\n",
    "              'missing': np.nan,\n",
    "              # when dart: 'rate_drop': (0.0, 1.0),\n",
    "              # 'grow_policy': 'depthwise','lossguide',\n",
    "              # 'verbosity': 1, #0\n",
    "              # 'base_score': 0.5 <- initial leaf prediction\n",
    "              \n",
    "              'eta': eta,\n",
    "              # 'gamma': gamma, # pruning phase of split in XGBoost unique tree\n",
    "              'max_depth': int(max_depth),\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'subsample': subsample, # 0.5 - randomly sample half of the training data prior to growing trees\n",
    "              'colsample_bytree': colsample_bytree, # subsample ratio of columns when constructing each tree\n",
    "              'colsample_bylevel': colsample_bylevel, # subsample ratio of columns for each level\n",
    "              'colsample_bynode': colsample_bynode, # subsample ratio of columns for each node (split)\n",
    "              'reg_lambda': reg_lambda,\n",
    "              'reg_alpha': reg_alpha}\n",
    "    \n",
    "    n_splits = len(split)\n",
    "    oofs = np.zeros(X_train.shape[0])\n",
    "    \n",
    "    models = []; learning_curves = []; valid_losses = []; valid_metrics = []\n",
    "    print(f'========== XGBoost Classifier training on: {X_train.shape} ==========')\n",
    "    for i, (train_idx, valid_idx) in enumerate(split):\n",
    "        print(f'========== XGBoost Classifier training: {i+1}/{n_splits} ==========')\n",
    "        train_d = xgb.DMatrix(data=X_train.loc[train_idx],\n",
    "                              label=y_train[train_idx],\n",
    "                              feature_names=common_cols+category_cols)\n",
    "                              #cat_features=category_cols)\n",
    "        valid_d = xgb.DMatrix(data=X_train.loc[valid_idx],\n",
    "                              label=y_train[valid_idx],\n",
    "                              feature_names=common_cols+category_cols)\n",
    "                              #cat_features=category_cols)\n",
    "        learning_curve = {}\n",
    "        model = xgb.train(params,\n",
    "                          train_d,\n",
    "                          evals=[(train_d, 'train'), (valid_d, 'valid')],\n",
    "                          num_boost_round=5000,\n",
    "                          early_stopping_rounds=20,\n",
    "                          verbose_eval=200,#False\n",
    "                          evals_result=learning_curve)\n",
    "        oofs[valid_idx] = model.predict(valid_d, ntree_limit=model.best_ntree_limit)\n",
    "        models.append(model)\n",
    "        learning_curves.append(learning_curve)\n",
    "        valid_metrics.append(model.best_score)\n",
    "          \n",
    "        del train_d, valid_d, model\n",
    "        gc.collect()\n",
    "        \n",
    "    print('====================')\n",
    "    print(f'CV AVG: {metric} - {np.mean(valid_metrics)}')\n",
    "    print(f'CV STD: {metric} - {np.std(valid_metrics)}')\n",
    "    print('====================')\n",
    "    \n",
    "    if bayes_opt:\n",
    "        return np.mean(valid_metrics)\n",
    "    else:\n",
    "        return oofs, models #, learning_curves\n",
    "\n",
    "def cb_clf_bayes_opt(init_points=20, n_iteration=80):\n",
    "    bounds = {'eta': (0.001, 0.3),\n",
    "              'max_depth': (3, 250),\n",
    "              'min_child_weight': (0, 100),\n",
    "              'subsample': (0.1, 1),\n",
    "              'colsample_bytree': (0.1, 1),\n",
    "              'colsample_bylevel': (0.1, 1),\n",
    "              'colsample_bynode': (0.1, 1),\n",
    "              'reg_lambda': (0, 300),\n",
    "              'reg_alpha': (0, 300)}\n",
    "    \n",
    "    optimizer = BayesianOptimization(f=cb_kfold_clf, pbounds=bounds, random_state=8982)\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iteration)\n",
    "    \n",
    "    print('Best score:', optimizer.max['target'])\n",
    "    print('Best set of parameters:')\n",
    "    print(optimizer.max['params'])\n",
    "    param = optimizer.max['params']; cv = optimizer.max['target']\n",
    "    return param, cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (Regression: max rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_kfold_reg(X_train, y_train, split, bayes_opt=True,\n",
    "                  eta=0.3, gamma=0, max_depth=6, min_child_weight=1,\n",
    "                  subsample=0.7, colsample_bytree=1.0, colsample_bylevel=1.0,\n",
    "                  colsample_bynode=1.0, reg_lambda=1.0, reg_alpha=0.0):\n",
    "    metric = 'rmse'\n",
    "    params = {'objective': 'reg:squarederror',\n",
    "              'eval_metric': metric,\n",
    "              'booster': 'gbtree', # 'dart',\n",
    "              'seed': 8982,\n",
    "\n",
    "              'missing': np.nan,\n",
    "              # when dart: 'rate_drop': (0.0, 1.0),\n",
    "              # 'grow_policy': 'depthwise','lossguide',\n",
    "              # 'verbosity': 1, #0\n",
    "              # 'base_score': 0.5 <- initial leaf prediction\n",
    "              \n",
    "              'eta': eta,\n",
    "              # 'gamma': gamma, # pruning phase of split in XGBoost unique tree\n",
    "              'max_depth': int(max_depth),\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'subsample': subsample, # 0.5 - randomly sample half of the training data prior to growing trees\n",
    "              'colsample_bytree': colsample_bytree, # subsample ratio of columns when constructing each tree\n",
    "              'colsample_bylevel': colsample_bylevel, # subsample ratio of columns for each level\n",
    "              'colsample_bynode': colsample_bynode, # subsample ratio of columns for each node (split)\n",
    "              'reg_lambda': reg_lambda,\n",
    "              'reg_alpha': reg_alpha}\n",
    "    \n",
    "    n_splits = len(split)\n",
    "    oofs = np.zeros(X_train.shape[0])\n",
    "    \n",
    "    models = []; learning_curves = []; valid_losses = []; valid_metrics = []\n",
    "    print(f'========== XGBoost Regressor training on: {X_train.shape} ==========')\n",
    "    for i, (train_idx, valid_idx) in enumerate(split):\n",
    "        print(f'========== XGBoost Regressor training: {i+1}/{n_splits} ==========')\n",
    "        train_d = xgb.DMatrix(data=X_train.loc[train_idx],\n",
    "                              label=y_train[train_idx],\n",
    "                              feature_names=common_cols+category_cols)\n",
    "                              #cat_features=category_cols)\n",
    "        valid_d = xgb.DMatrix(data=X_train.loc[valid_idx],\n",
    "                              label=y_train[valid_idx],\n",
    "                              feature_names=common_cols+category_cols)\n",
    "                              #cat_features=category_cols)\n",
    "        learning_curve = {}\n",
    "        model = xgb.train(params,\n",
    "                          train_d,\n",
    "                          evals=[(train_d, 'train'), (valid_d, 'valid')],\n",
    "                          num_boost_round=5000,\n",
    "                          early_stopping_rounds=20,\n",
    "                          verbose_eval=200,#False\n",
    "                          evals_result=learning_curve)\n",
    "        oofs[valid_idx] = model.predict(valid_d, ntree_limit=model.best_ntree_limit)\n",
    "        models.append(model)\n",
    "        learning_curves.append(learning_curve)\n",
    "        valid_metrics.append(model.best_score)\n",
    "          \n",
    "        del train_d, valid_d, model\n",
    "        gc.collect()\n",
    "        \n",
    "    print('====================')\n",
    "    print(f'CV AVG: {metric} - {np.mean(valid_metrics)}')\n",
    "    print(f'CV STD: {metric} - {np.std(valid_metrics)}')\n",
    "    print('====================')\n",
    "    \n",
    "    if bayes_opt:\n",
    "        return -np.mean(valid_metrics)\n",
    "    else:\n",
    "        return oofs, models #, learning_curves\n",
    "\n",
    "def xgb_clf_bayes_opt(init_points=20, n_iteration=80):\n",
    "    bounds = {'eta': (0.001, 0.3),\n",
    "              'max_depth': (3, 250),\n",
    "              'min_child_weight': (0, 100),\n",
    "              'subsample': (0.1, 1),\n",
    "              'colsample_bytree': (0.1, 1),\n",
    "              'colsample_bylevel': (0.1, 1),\n",
    "              'colsample_bynode': (0.1, 1),\n",
    "              'reg_lambda': (0, 300),\n",
    "              'reg_alpha': (0, 300)}\n",
    "    \n",
    "    optimizer = BayesianOptimization(f=cb_kfold_clf, pbounds=bounds, random_state=8982)\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iteration)\n",
    "    \n",
    "    print('Best score:', -optimizer.max['target'])\n",
    "    print('Best set of parameters:')\n",
    "    print(optimizer.max['params'])\n",
    "    param = optimizer.max['params']; cv = -optimizer.max['target']\n",
    "    return param, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_pred(X_test, models):\n",
    "    y_pred_test_total = np.zeros(X_test.shape[0])\n",
    "    test_d = xgb.DMatrix(X_test)\n",
    "    for i, model in enumerate(models):\n",
    "        print(f'========== XGBoost Predicting with {i+1}-th model ==========')\n",
    "        y_pred_test = model.predict(test_d, ntree_limit=model.best_ntree_limit)\n",
    "        y_pred_test_total += y_pred_test\n",
    "    y_pred_test_total /= len(models)\n",
    "    return y_pred_test_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neuralnet(\n",
    "    recipe,\n",
    "    loss='mse',\n",
    "    optimizer='adam',\n",
    "    lr=1e-3,\n",
    "    monitor='val_loss',\n",
    "    es_patience=-1,\n",
    "    restore_best_weights=True,\n",
    "    lr_scheduler='none',\n",
    "    lr_factor=0.1,\n",
    "    lr_patience=5,\n",
    "    seed=8982,\n",
    "    **_,\n",
    "):\n",
    "    tf.set_random_seed(seed)\n",
    "    model = keras.models.model_from_json(recipe)\n",
    "    \n",
    "    if loss == 'mse':\n",
    "        loss = keras.losses.mean_squared_error\n",
    "    elif loss == 'bce':\n",
    "        loss = keras.losses.binary_crossentropy\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(lr)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    \n",
    "    callbacks = []\n",
    "    \n",
    "    if es_patience >= 0:\n",
    "        es = keras.callbacks.EarlyStopping(monitor=monitor,\n",
    "                                           patience=es_patience,\n",
    "                                           restore_best_weights=restore_best_weights,\n",
    "                                           verbose=1)\n",
    "        callbacks.append(es)\n",
    "    \n",
    "    if lr_scheduler == 'none':\n",
    "        pass\n",
    "    elif lr_scheduler == 'reduce_on_plateau':\n",
    "        lr_sche = keras.callbacks.ReduceLROnPlateau(monitor=monitor,\n",
    "                                                    factor=lr_factor,\n",
    "                                                    patience=lr_patience,\n",
    "                                                    verbose=1)\n",
    "        callbacks.append(lr_sche)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    return model, callbacks\n",
    "\n",
    "\n",
    "def train_neuralnet(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=None,\n",
    "):\n",
    "    model, callbacks = build_neuralnet(**params)\n",
    "    model.fit(X_train,\n",
    "              y_train,\n",
    "              validation_data=validation_data,\n",
    "              batch_size=params['batch_size'],\n",
    "              epochs=params['epochs'],\n",
    "              callbacks=callbacks)\n",
    "    return model\n",
    "\n",
    "def run_kfold_neuralnet(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    cv,\n",
    "    features,\n",
    "    metrics,\n",
    "):\n",
    "    oof = np.zeros(len(X_train))\n",
    "    predictions = np.zeros(len(X_test))\n",
    "    \n",
    "    n_splits = len(cv)\n",
    "    print(f\"k={n_splits} folds neuralnet running...\")\n",
    "    print(f\"train data/feature shape: {X_train[features].shape}\")\n",
    "    \n",
    "    for fold, (dev_idx, val_idx) in enumerate(cv):\n",
    "        validation_data = [X_train.loc[val_idx, features], y_train[val_idx]]\n",
    "        model = train_neuralnet(params,\n",
    "                                X_train.loc[dev_idx, features],\n",
    "                                y_train[dev_idx],\n",
    "                                validation_data=validation_data)\n",
    "        \n",
    "        oof[val_idx] = model.predict(X_train.loc[val_idx, features].values)[:,0]\n",
    "        predictions += model.predict(X_test[features].values)[:,0] / n_splits\n",
    "        \n",
    "        msg = f'fold: {fold}'\n",
    "        for name, func in metrics.items():\n",
    "            score = func(y_train[val_idx], oof[val_idx])\n",
    "            msg += f' - {name}: {score:.5f}'\n",
    "        print(msg)\n",
    "    \n",
    "    msg = f'CV score'\n",
    "    for name, func in metrics.items():\n",
    "        score = func(y_train, oof)\n",
    "        msg += f' - {name}: {score:.5f}'\n",
    "    print(msg)\n",
    "\n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat                0\n",
      "label              0\n",
      "cat_MEAN           0\n",
      "cat_SMTH_MEAN_1    0\n",
      "cat_SMTH_MEAN_5    0\n",
      "cat_MAX            0\n",
      "cat_MIN            0\n",
      "cat_RNG            0\n",
      "cat_STD            0\n",
      "cat_Q1             0\n",
      "cat_Q2             0\n",
      "cat_Q3             0\n",
      "cat_IQR            0\n",
      "dtype: int64\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_15 (Batc (None, 12)                48        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               3328      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "p_re_lu_10 (PReLU)           (None, 256)               256       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "p_re_lu_11 (PReLU)           (None, 256)               256       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 71,985\n",
      "Trainable params: 70,937\n",
      "Non-trainable params: 1,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "features_nn = list(set(x_tr.columns) - {'label'})\n",
    "x_tr['cat_STD'].fillna(0, inplace=True)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(len(features_nn,))),\n",
    "    keras.layers.BatchNormalization(),\n",
    "  \n",
    "    keras.layers.Dense(256),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "  \n",
    "    keras.layers.Dense(256),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "     keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat_SMTH_MEAN_5', 'cat_Q3', 'cat_MIN', 'cat_STD', 'cat_Q2', 'cat_RNG', 'cat_IQR', 'cat_MAX', 'cat_SMTH_MEAN_1', 'cat_MEAN', 'cat', 'cat_Q1']\n",
      "k=2 folds neuralnet running...\n",
      "train data/feature shape: (6, 12)\n",
      "Train on 2 samples, validate on 4 samples\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 553ms/sample - loss: 0.6931 - val_loss: 0.6915\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 9ms/sample - loss: 0.6931 - val_loss: 0.6913\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 5ms/sample - loss: 0.6931 - val_loss: 0.6912\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 9ms/sample - loss: 0.6931 - val_loss: 0.6910\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 4ms/sample - loss: 0.6931 - val_loss: 0.6909\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6907\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 12ms/sample - loss: 0.6931 - val_loss: 0.6906\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6905\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6904\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6902\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 11ms/sample - loss: 0.6931 - val_loss: 0.6899\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6897\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6896\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6894\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 7ms/sample - loss: 0.6931 - val_loss: 0.6892\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6890\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6888\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6886\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6883\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6881\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 15ms/sample - loss: 0.6931 - val_loss: 0.6879\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6878\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6876\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6875\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 10ms/sample - loss: 0.6931 - val_loss: 0.6874\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6873\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6871\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 9ms/sample - loss: 0.6931 - val_loss: 0.6869\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 16ms/sample - loss: 0.6931 - val_loss: 0.6867\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 16ms/sample - loss: 0.6931 - val_loss: 0.6865\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6862\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6859\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6856\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6854\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6850\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6846\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6841\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6836\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 8ms/sample - loss: 0.6931 - val_loss: 0.6831\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6827\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6823\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 8ms/sample - loss: 0.6931 - val_loss: 0.6819\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6815\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6811\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6806\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6801\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 8ms/sample - loss: 0.6931 - val_loss: 0.6796\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6791\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 12ms/sample - loss: 0.6931 - val_loss: 0.6786\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 15ms/sample - loss: 0.6931 - val_loss: 0.6780\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6774\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6769\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6763\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 7ms/sample - loss: 0.6931 - val_loss: 0.6757\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6752\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 12ms/sample - loss: 0.6931 - val_loss: 0.6747\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6741\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 20ms/sample - loss: 0.6931 - val_loss: 0.6737\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 9ms/sample - loss: 0.6931 - val_loss: 0.6732\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6728\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6723\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6719\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6713\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6708\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6703\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6699\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6694\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6689\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6684\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6679\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6674\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6669\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6664\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6659\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6654\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6650\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6646\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6642\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6638\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6633\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6629\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6625\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 17ms/sample - loss: 0.6931 - val_loss: 0.6621\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6617\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6612\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6608\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 4ms/sample - loss: 0.6931 - val_loss: 0.6604\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6600\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 17ms/sample - loss: 0.6931 - val_loss: 0.6596\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6592\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6588\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6584\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 6ms/sample - loss: 0.6931 - val_loss: 0.6580\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6576\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6571\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6566\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6931 - val_loss: 0.6562\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 6ms/sample - loss: 0.6931 - val_loss: 0.6558\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6554\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6931 - val_loss: 0.6550\n",
      "fold: 0 - auc_ome3: 0.62500\n",
      "Train on 4 samples, validate on 2 samples\n",
      "Epoch 1/100\n",
      "4/4 [==============================] - 1s 293ms/sample - loss: 0.9460 - val_loss: 0.7050\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.4325 - val_loss: 0.7012\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.5896 - val_loss: 0.6980\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 7ms/sample - loss: 0.2837 - val_loss: 0.6957\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.3961 - val_loss: 0.6944\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.3579 - val_loss: 0.6936\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.3193 - val_loss: 0.6933\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 3ms/sample - loss: 0.4815 - val_loss: 0.6932\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.4868 - val_loss: 0.6932\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.4681 - val_loss: 0.6932\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 5ms/sample - loss: 0.2161 - val_loss: 0.6934\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.3928 - val_loss: 0.6936\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "4/4 [==============================] - 0s 38ms/sample - loss: 0.3028 - val_loss: 0.6938\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.3773 - val_loss: 0.6937\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.3457 - val_loss: 0.6937\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.3244 - val_loss: 0.6937\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 875us/sample - loss: 0.4321 - val_loss: 0.6937\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.2776 - val_loss: 0.6936\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.4680 - val_loss: 0.6936\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.2399 - val_loss: 0.6936\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.4140 - val_loss: 0.6935\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 6ms/sample - loss: 0.4374 - val_loss: 0.6934\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.2938 - val_loss: 0.6934\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 7ms/sample - loss: 0.4666 - val_loss: 0.6933\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.3460 - val_loss: 0.6933\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 875us/sample - loss: 0.2559 - val_loss: 0.6932\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.2701 - val_loss: 0.6932\n",
      "Epoch 28/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "4/4 [==============================] - 0s 45ms/sample - loss: 0.3670 - val_loss: 0.6932\n",
      "Epoch 00028: early stopping\n",
      "fold: 1 - auc_ome3: 0.50000\n",
      "CV score - auc_ome3: 0.66667\n"
     ]
    }
   ],
   "source": [
    "SEED = 8982\n",
    "\n",
    "params = {\n",
    "    'recipe': model.to_json(),\n",
    "    'optimizer': 'adam',\n",
    "    'lr': 1e-3,\n",
    "    'loss': 'bce',\n",
    "    'monitor': 'val_loss',\n",
    "    'es_patience': 20,\n",
    "    'restore_best_weights': True,\n",
    "    'lr_scheduler': 'reduce_on_plateau',\n",
    "    'lr_factor': 0.1,\n",
    "    'lr_patience': 5,\n",
    "    'seed': SEED,\n",
    " \n",
    "    'epochs': 100,\n",
    "    'batch_size': 64,\n",
    "}\n",
    "print(features_nn)\n",
    "X_train_nn = x_tr[features_nn]\n",
    "y_train_nn = x_tr['label'].values\n",
    "X_test_nn = x_te[features_nn]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "metrics = {'auc_ome3': roc_auc_score}\n",
    "\n",
    "seed_list = SEED + np.arange(3)\n",
    "oof_nn, predictions_nn = run_kfold_neuralnet(params,\n",
    "                                               X_train_nn,\n",
    "                                               y_train_nn,\n",
    "                                               X_test_nn,\n",
    "                                             build_cv_spliter(X_train_nn, y_train_nn, n_splits=2),\n",
    "                                               features_nn,\n",
    "                                               metrics)\n",
    "                                               #seed_list=seed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(\n",
    "    feature_importance_df,\n",
    "    feature_name='feature',\n",
    "    importance_name=['split', 'gain'],\n",
    "    top_k=50,\n",
    "    fig_width=16,\n",
    "    fig_height=8,\n",
    "    fontsize=14,\n",
    "):\n",
    "    if isinstance(importance_name, str):\n",
    "        importance_name = [importance_name]\n",
    "    \n",
    "    num_importance = len(importance_name)\n",
    "    plt.figure(figsize=(fig_width, fig_height*num_importance))\n",
    "    gs = gridspec.GridSpec(1, num_importance)\n",
    "    \n",
    "    def _fetch_best_features(df, fimp='gain'):\n",
    "        cols = (df[[feature_name, fimp]]\n",
    "                .groupby(feature_name)\n",
    "                .mean()\n",
    "                .sort_values(by=fimp, ascending=False)\n",
    "                .index\n",
    "                .values[:top_k])\n",
    "        return cols, df.loc[df[feature_name].isin(cols)]\n",
    "    \n",
    "    for i, fimp in enumerate(importance_name):\n",
    "        cols, best_features = _fetch_best_features(feature_importance_df, fimp)\n",
    "        ax = plt.subplot(gs[0, i])\n",
    "        sns.barplot(x=fimp, y=feature_name, data=best_features, order=cols, ax=ax)\n",
    "        title = f'Features {fimp} importance (averaged/folds)'\n",
    "        plt.title(title, fontweight='bold', fontsize=fontsize)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "# or this\n",
    "# fold_importance_df.plot.barh(x='feature', y='gain', figsize=(13,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Feature Elimination by LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Iterative_CV:\n",
    "    def __init__(self, X_train_full, y_train, eval_cols, metric):\n",
    "        self.X_train_full = X_train_full\n",
    "        self.y_train = y_train\n",
    "        self.eval_cols = eval_cols\n",
    "        self.metric = metric\n",
    "    \n",
    "    # =====\n",
    "    # eliminates/imputes one feature at a time,\n",
    "    # returns list with options and discards\n",
    "    # ====\n",
    "    \n",
    "    def iter_cv_elim():\n",
    "        excl_improve = []; excl_worse = []\n",
    "        if self.metric == 'rmse':\n",
    "            init_valid_avg_score = -1 * lgb_kfold(self.X_train_full, self.y_train, bayes_opt=True)\n",
    "            print(f'[Iter_Feature_Elim] Current best score is {init_valid_avg_score}')\n",
    "            for cols in tqdm(self.eval_cols):\n",
    "                temp_cols = list(set(self.X_train_full.columns) - {col})\n",
    "                X_train = self.X_train_full[temp_cols]\n",
    "                new_valid_avg_score = -1 * lgb_kfold(X_train, self.y_train, bayes_opt=True)\n",
    "                degree = new_valid_avg_score - init_valid_avg_score\n",
    "                if degree < 0:\n",
    "                    pct = 100 * (-1 * degree / init_valid_avg_score)\n",
    "                    excl_improve.append([col, pct])\n",
    "                    print(f\"[Iter_Feature_Elim] '{col}', exclusion improved (lowered) avg CV by {pct}pct.\")\n",
    "                else:\n",
    "                    pct = 100 * (degree / init_valid_avg_score)\n",
    "                    excl_worse.append([col, pct])\n",
    "                    print(f\"[Iter_Feature_Elim] '{col}', exclusion worsened (raised) avg CV by {pct}pct.\")\n",
    "        elif self.metric == 'auc':\n",
    "            init_valid_avg_score = lgb_skfold(self.X_train_full, self.y_train, bayes_opt=True)\n",
    "            print(f'[Iter_Feature_Elim] Current best score is {init_valid_avg_score}')\n",
    "            for col in tqdm(self.eval_cols):\n",
    "                temp_cols = list(set(self.X_train_full.columns) - {col})\n",
    "                X_train = self.X_train_full[temp_cols]\n",
    "                new_valid_avg_score = lgb_skfold(X_train, self.y_train, bayes_opt=True)\n",
    "                degree = new_valid_avg_score - init_valid_avg_score\n",
    "                if degree > 0:\n",
    "                    pct = 100 * (degree / init_valid_avg_score)\n",
    "                    excl_improve.append([col, pct])\n",
    "                    print(f\"[Iter_Feature_Elim] '{col}', exclusion improved (raised) avg CV by {pct}pct.\")\n",
    "                else:\n",
    "                    pct = 100 * (-1 * degree / init_valid_avg_score)\n",
    "                    excl_worse.append([col, pct])\n",
    "                    print(f\"[Iter_Feature_Elim] '{col}', exclusion worsened (lowered) avg CV by {pct}pct.\")\n",
    "\n",
    "        excl_improve.sort(key=lambda lst: lst[1])\n",
    "        excl_worse.sort(key=lambda lst: lst[1])\n",
    "        del init_valid_avg_score, cols, temp_cols, X_train, new_valid_avg_score, degree, pct\n",
    "        gc.collect()\n",
    "        return excl_improve, excl_worse\n",
    "    \n",
    "    def iter_cv_rank():\n",
    "        impt = []\n",
    "        if self.metric == 'rmse':\n",
    "            for col in tqdm(self.eval_cols):\n",
    "                X_train = self.X_train_full[col]\n",
    "                assert X_train.shape[1] == 1\n",
    "                print(f\"[Iter_Feature_Rank] '{col}', evaluation ongoing.\")\n",
    "                valid_avg_score = -1 * lgb_kfold(X_train, self.y_train, bayes_opt=True)\n",
    "                impt.append([col, valid_avg_score])\n",
    "        elif self.metric == 'auc':\n",
    "            for col in tqdm(self.eval_cols):\n",
    "                X_train = self.X_train_full[col]\n",
    "                assert X_train.shape[1] == 1\n",
    "                print(f\"[Iter_Feature_Rank] '{col}', evaluation ongoing.\")\n",
    "                valid_avg_score = lgb_skfold(X_train, self.y_train, bayes_opt=True)\n",
    "                impt.append([col, valid_avg_score])\n",
    "        impt.sort(key=lambda lst: lst[1])\n",
    "        del col, X_train, valid_avg_score\n",
    "        gc.collect()\n",
    "        return impt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Importance Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_lgb_fimp(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    features,\n",
    "    shuffle,\n",
    "    seed=42,\n",
    "    categorical=[]\n",
    "):\n",
    "    # Shuffle target if required\n",
    "    y = y_train.copy()\n",
    "    if shuffle:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        y = y_train.copy().sample(frac=1.0)\n",
    "    \n",
    "    arg_categorical = categorical if len(categorical) > 0 else 'auto'\n",
    "    dtrain = lgb.Dataset(X_train[features],\n",
    "                         label=y.values,\n",
    "                         categorical_feature=arg_categorical)\n",
    "    \n",
    "    # Fit the model\n",
    "    clf = lgb.train(params, dtrain)\n",
    "\n",
    "    # Get feature importances\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = features\n",
    "    imp_df['split'] = clf.feature_importance(importance_type='split')\n",
    "    imp_df['gain'] = clf.feature_importance(importance_type='gain')\n",
    "    \n",
    "    return imp_df\n",
    "\n",
    "\n",
    "def null_importance_selection(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    features,\n",
    "    seed=42,\n",
    "    categorical=[],\n",
    "    num_actual_run=1,\n",
    "    num_null_run=40,\n",
    "    eps=1e-10,\n",
    "    valid_percentile=75,\n",
    "):\n",
    "    actual_imp_df = pd.DataFrame()\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    for i in tqdm(range(num_actual_run)):\n",
    "        seed = np.random.randint(1000)\n",
    "        imp_df = _get_lgb_fimp(params,\n",
    "                               X_train,\n",
    "                               y_train,\n",
    "                               features,\n",
    "                               shuffle=False,\n",
    "                               seed=seed,\n",
    "                               categorical=categorical)\n",
    "        imp_df['run'] = i\n",
    "        actual_imp_df = pd.concat([actual_imp_df, imp_df], axis=0)\n",
    "    \n",
    "    null_imp_df = pd.DataFrame()\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    for i in tqdm(range(num_null_run)):\n",
    "        seed = np.random.randint(1000)\n",
    "        imp_df = _get_lgb_fimp(params,\n",
    "                               X_train,\n",
    "                               y_train,\n",
    "                               features,\n",
    "                               shuffle=True,\n",
    "                               seed=seed,\n",
    "                               categorical=categorical)\n",
    "        imp_df['run'] = i\n",
    "        null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n",
    "    \n",
    "    feature_scores = []\n",
    "    \n",
    "    for _f in actual_imp_df['feature'].unique():\n",
    "        # importance gain of gain\n",
    "        act_fimp_split = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'split'].mean()\n",
    "        null_fimp_split = null_imp_df.loc[null_imp_df['feature'] == _f, 'split'].values\n",
    "        split_score = np.log(eps + act_fimp_split / (1 + np.percentile(null_fimp_split, valid_percentile)))\n",
    "        \n",
    "        # importance gain of gain\n",
    "        act_fimp_gain = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'gain'].mean()\n",
    "        null_fimp_gain = null_imp_df.loc[null_imp_df['feature'] == _f, 'gain'].values\n",
    "        gain_score = np.log(eps + act_fimp_gain / (1 + np.percentile(null_fimp_gain, valid_percentile)))\n",
    "\n",
    "        feature_scores.append((_f, split_score, gain_score))\n",
    "    \n",
    "    scores_df = pd.DataFrame(feature_scores, columns=['feature', 'split_score', 'gain_score'])\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_high_corr_columns(df, threshold=0.99, verbose=True):\n",
    "    df_corr = abs(df.corr())\n",
    "    delete_columns = []\n",
    "    \n",
    "    # diagonal values filled by zero\n",
    "    for i in range(0, len(df_corr.columns)):\n",
    "        df_corr.iloc[i, i] = 0\n",
    "    \n",
    "    # loop as removing high-correlated columns in df_corr\n",
    "    while True:\n",
    "        df_max_column_value = df_corr.max()\n",
    "        max_corr = df_max_column_value.max()\n",
    "        query_column = df_max_column_value.idxmax()\n",
    "        target_column = df_corr[query_column].idxmax()\n",
    "        \n",
    "        if max_corr < threshold:\n",
    "            break\n",
    "        else:\n",
    "            # drop feature which is highly correlated with others \n",
    "            if sum(df_corr[query_column]) <= sum(df_corr[target_column]):\n",
    "                delete_column = target_column\n",
    "                saved_column = query_column\n",
    "            else:\n",
    "                delete_column = query_column\n",
    "                saved_column = target_column\n",
    "            \n",
    "            df_corr.drop([delete_column], axis=0, inplace=True)\n",
    "            df_corr.drop([delete_column], axis=1, inplace=True)\n",
    "            delete_columns.append(delete_column)\n",
    "            \n",
    "            if verbose:\n",
    "                printl('{}: Drop: {} <- Query: {}, Corr: {:.5f}'.format(\n",
    "                    len(delete_columns), delete_column, saved_column, max_corr\n",
    "                ))\n",
    "\n",
    "    return delete_columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
